{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Jandsy/ml_finance_imperial/blob/main/Coursework/CourseWork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSIsUTeyXNr_"
   },
   "source": [
    "# **<center>Machine Learning and Finance </center>**\n",
    "\n",
    "\n",
    "## <center> CourseWork 2024 - StatArb </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBDj0uD44dQL"
   },
   "source": [
    "\n",
    "In this coursework, you will delve into and replicate selected elements of the research detailed in the paper **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**. **However, we will not reproduce the entire study.**\n",
    "\n",
    "## Overview\n",
    "\n",
    "This study redefines Statistical Arbitrage (StatArb) by combining Autoencoder architectures and policy learning to generate trading strategies. Traditionally, StatArb involves finding the mean of a synthetic asset through classical or PCA-based methods before developing a mean reversion strategy. However, this paper proposes a data-driven approach using an Autoencoder trained on US stock returns, integrated into a neural network representing portfolio trading policies to output portfolio allocations directly.\n",
    "\n",
    "\n",
    "## Coursework Goal\n",
    "\n",
    "This coursework will replicate these results, providing hands-on experience in implementing and evaluating this innovative end-to-end policy learning Autoencoder within financial trading strategies.\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [Data Preparation and Exploration](#Data-Preparation-and-Exploration)\n",
    "- [Fama French Analysis](#Fama-French-Analysis)\n",
    "- [PCA Analysis](#PCA-Analysis)\n",
    "- [Ornstein Uhlenbeck](#Ornstein-Uhlenbeck)\n",
    "- [Autoencoder Analysis](#Autoencoder-Analysis)\n",
    "\n",
    "\n",
    "\n",
    "**Description:**\n",
    "The Coursework is graded on a 100 point scale and is divided into five  parts. Below is the mark distribution for each question:\n",
    "\n",
    "| **Problem**  | **Question**          | **Number of Marks** |\n",
    "|--------------|-----------------------|---------------------|\n",
    "| **Part A**   | Question 1            | 4                   |\n",
    "|              | Question 2            | 1                   |\n",
    "|              | Question 3            | 3                   |\n",
    "|              | Question 4            | 3                   |\n",
    "|              | Question 5            | 1                   |\n",
    "|              | Question 6            | 3                   |\n",
    "|**Part  B**    | Question 7           | 1                   |\n",
    "|              | Question 8            | 5                   |\n",
    "|              | Question 9            | 4                   |\n",
    "|              | Question 10           | 5                   |\n",
    "|              | Question 11           | 2                   |\n",
    "|              | Question 12           | 3                   |\n",
    "|**Part  C**    | Question 13          | 3                   |\n",
    "|              | Question 14           | 1                   |\n",
    "|              | Question 15           | 3                   |\n",
    "|              | Question 16           | 2                   |\n",
    "|              | Question 17           | 7                   |\n",
    "|              | Question 18           | 6                   |\n",
    "|              | Question 19           | 3                   |\n",
    "|  **Part  D** | Question 20           | 3                   |\n",
    "|              | Question 21           | 5                   |\n",
    "|              | Question 22           | 2                   |\n",
    "|  **Part  E** | Question 23           | 2                   |\n",
    "|              | Question 24           | 1                   |\n",
    "|              | Question 25           | 3                   |\n",
    "|              | Question 26           | 10                  |\n",
    "|              | Question 27           | 1                   |\n",
    "|              | Question 28           | 3                   |\n",
    "|              | Question 29           | 3                   |\n",
    "|              | Question 30           | 7                   |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Please read the questions carefully and do your best. Good luck!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "\n",
    "\n",
    "## 1. Data Preparation and Exploration\n",
    "Collect, clean, and prepare US stock return data for analysis.\n",
    "\n",
    "## 2. Fama French Analysis\n",
    "Utilize Fama French Factors to isolate the idiosyncratic components of stock returns, differentiating them from market-wide effects. This analysis helps in understanding the unique characteristics of individual stocks relative to broader market trends.\n",
    "\n",
    "## 3. PCA Analysis\n",
    "Employ Principal Component Analysis (PCA) to identify hidden structures and reduce dimensionality in the data. This method helps in extracting significant patterns that might be obscured in high-dimensional datasets.\n",
    "\n",
    "## 4. Ornstein-Uhlenbeck Process\n",
    "Analyze mean-reverting behavior in stock prices using the Ornstein-Uhlenbeck process. This stochastic process is useful for modeling and forecasting based on the assumption that prices will revert to a long-term mean.\n",
    "\n",
    "## 5. Building a Basic Autoencoder Model\n",
    "Construct and train a standard Autoencoder to extract residual idiosyncratic risk.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFU9ckGplGDf"
   },
   "source": [
    "# Data Preparation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiqyU4DQlxsV"
   },
   "source": [
    "\n",
    "---\n",
    "<font color=green>Q1: (4 Marks)</font>\n",
    "<br><font color='green'>\n",
    "Write a Python function that accepts a URL parameter and retrieves the NASDAQ-100 companies and their ticker symbols by scraping the relevant Wikipedia page using **[Requests](https://pypi.org/project/requests/)** and **[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)**. Your function should return the data as a list of tuples, with each tuple containing the company name and its ticker symbol. Then, call your function with the appropriate Wikipedia page URL and print the data in a 'Company: Ticker' format.\n",
    "\n",
    "</font>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lcJXF6aSxCGu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adobe Inc.: ADBE\n",
      "ADP: ADP\n",
      "Airbnb: ABNB\n",
      "Alphabet Inc. (Class A): GOOGL\n",
      "Alphabet Inc. (Class C): GOOG\n",
      "Amazon: AMZN\n",
      "Advanced Micro Devices Inc.: AMD\n",
      "American Electric Power: AEP\n",
      "Amgen: AMGN\n",
      "Analog Devices: ADI\n",
      "Ansys: ANSS\n",
      "Apple Inc.: AAPL\n",
      "Applied Materials: AMAT\n",
      "ASML Holding: ASML\n",
      "AstraZeneca: AZN\n",
      "Atlassian: TEAM\n",
      "Autodesk: ADSK\n",
      "Baker Hughes: BKR\n",
      "Biogen: BIIB\n",
      "Booking Holdings: BKNG\n",
      "Broadcom Inc.: AVGO\n",
      "Cadence Design Systems: CDNS\n",
      "CDW Corporation: CDW\n",
      "Charter Communications: CHTR\n",
      "Cintas: CTAS\n",
      "Cisco: CSCO\n",
      "Coca-Cola Europacific Partners: CCEP\n",
      "Cognizant: CTSH\n",
      "Comcast: CMCSA\n",
      "Constellation Energy: CEG\n",
      "Copart: CPRT\n",
      "CoStar Group: CSGP\n",
      "Costco: COST\n",
      "CrowdStrike: CRWD\n",
      "CSX Corporation: CSX\n",
      "Datadog: DDOG\n",
      "DexCom: DXCM\n",
      "Diamondback Energy: FANG\n",
      "Dollar Tree: DLTR\n",
      "DoorDash: DASH\n",
      "Electronic Arts: EA\n",
      "Exelon: EXC\n",
      "Fastenal: FAST\n",
      "Fortinet: FTNT\n",
      "GE HealthCare: GEHC\n",
      "Gilead Sciences: GILD\n",
      "GlobalFoundries: GFS\n",
      "Honeywell: HON\n",
      "Idexx Laboratories: IDXX\n",
      "Illumina, Inc.: ILMN\n",
      "Intel: INTC\n",
      "Intuit: INTU\n",
      "Intuitive Surgical: ISRG\n",
      "Keurig Dr Pepper: KDP\n",
      "KLA Corporation: KLAC\n",
      "Kraft Heinz: KHC\n",
      "Lam Research: LRCX\n",
      "Linde plc: LIN\n",
      "Lululemon: LULU\n",
      "Marriott International: MAR\n",
      "Marvell Technology: MRVL\n",
      "MercadoLibre: MELI\n",
      "Meta Platforms: META\n",
      "Microchip Technology: MCHP\n",
      "Micron Technology: MU\n",
      "Microsoft: MSFT\n",
      "Moderna: MRNA\n",
      "MondelÄ“z International: MDLZ\n",
      "MongoDB Inc.: MDB\n",
      "Monster Beverage: MNST\n",
      "Netflix: NFLX\n",
      "Nvidia: NVDA\n",
      "NXP: NXPI\n",
      "O'Reilly Automotive: ORLY\n",
      "Old Dominion Freight Line: ODFL\n",
      "Onsemi: ON\n",
      "Paccar: PCAR\n",
      "Palo Alto Networks: PANW\n",
      "Paychex: PAYX\n",
      "PayPal: PYPL\n",
      "PDD Holdings: PDD\n",
      "PepsiCo: PEP\n",
      "Qualcomm: QCOM\n",
      "Regeneron: REGN\n",
      "Roper Technologies: ROP\n",
      "Ross Stores: ROST\n",
      "Sirius XM: SIRI\n",
      "Starbucks: SBUX\n",
      "Synopsys: SNPS\n",
      "Take-Two Interactive: TTWO\n",
      "T-Mobile US: TMUS\n",
      "Tesla, Inc.: TSLA\n",
      "Texas Instruments: TXN\n",
      "The Trade Desk: TTD\n",
      "Verisk: VRSK\n",
      "Vertex Pharmaceuticals: VRTX\n",
      "Walgreens Boots Alliance: WBA\n",
      "Warner Bros. Discovery: WBD\n",
      "Workday, Inc.: WDAY\n",
      "Xcel Energy: XEL\n",
      "Zscaler: ZS\n",
      "The dataset is a tuple.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to get NASDAQ-100 companies and their tickers\n",
    "def get_nasdaq100_companies(nasdaq100):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(nasdaq100)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the response using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find the table with the ID 'constituents'\n",
    "        table = soup.find('table', {'id': 'constituents'})\n",
    "        \n",
    "        # Extract and return the company names and tickers as a tuple\n",
    "        return tuple((row.find_all('td')[0].text.strip(), row.find_all('td')[1].text.strip())\n",
    "                     for row in table.find_all('tr')[1:])\n",
    "    else:\n",
    "        # Print an error message if the request failed\n",
    "        print(\"Failed to retrieve data from the URL.\")\n",
    "        return None\n",
    "\n",
    "# Wikipedia URL containing NASDAQ-100 companies\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/NASDAQ-100\"\n",
    "# Get the NASDAQ-100 companies and their tickers\n",
    "nasdaq100_data = get_nasdaq100_companies(wiki_url)\n",
    "\n",
    "# If data was successfully retrieved, print each company and its ticker\n",
    "if nasdaq100_data:\n",
    "    # Create a list of tickers for further use if needed\n",
    "    tickers_list = [company[1] for company in nasdaq100_data]\n",
    "    for company in nasdaq100_data:\n",
    "        print(f\"{company[0]}: {company[1]}\")\n",
    "\n",
    "# Double-check if the dataset is in tuple format\n",
    "if isinstance(nasdaq100_data, tuple):\n",
    "    print(\"The dataset is a tuple.\")\n",
    "else:\n",
    "    print(\"The dataset is not a tuple.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VvyNAF9sE7u"
   },
   "source": [
    "---\n",
    "<font color=green>Q2: (1 Mark)</font>\n",
    "<br><font color='green'>\n",
    "Given a list of tuples representing NASDAQ-100 companies (where each tuple contains a company name and its ticker symbol), write a Python script to extract all ticker symbols into a separate list called `tickers_list`.\n",
    "</font>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yQaWckkXxDLP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADBE', 'ADP', 'ABNB', 'GOOGL', 'GOOG', 'AMZN', 'AMD', 'AEP', 'AMGN', 'ADI', 'ANSS', 'AAPL', 'AMAT', 'ASML', 'AZN', 'TEAM', 'ADSK', 'BKR', 'BIIB', 'BKNG', 'AVGO', 'CDNS', 'CDW', 'CHTR', 'CTAS', 'CSCO', 'CCEP', 'CTSH', 'CMCSA', 'CEG', 'CPRT', 'CSGP', 'COST', 'CRWD', 'CSX', 'DDOG', 'DXCM', 'FANG', 'DLTR', 'DASH', 'EA', 'EXC', 'FAST', 'FTNT', 'GEHC', 'GILD', 'GFS', 'HON', 'IDXX', 'ILMN', 'INTC', 'INTU', 'ISRG', 'KDP', 'KLAC', 'KHC', 'LRCX', 'LIN', 'LULU', 'MAR', 'MRVL', 'MELI', 'META', 'MCHP', 'MU', 'MSFT', 'MRNA', 'MDLZ', 'MDB', 'MNST', 'NFLX', 'NVDA', 'NXPI', 'ORLY', 'ODFL', 'ON', 'PCAR', 'PANW', 'PAYX', 'PYPL', 'PDD', 'PEP', 'QCOM', 'REGN', 'ROP', 'ROST', 'SIRI', 'SBUX', 'SNPS', 'TTWO', 'TMUS', 'TSLA', 'TXN', 'TTD', 'VRSK', 'VRTX', 'WBA', 'WBD', 'WDAY', 'XEL', 'ZS']\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "\n",
    "tickers_list = [company[1] for company in nasdaq100_data]\n",
    "\n",
    "print(tickers_list)\n",
    "\n",
    "#Double check if all companies are called\n",
    "print(len(tickers_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KEkAPUxsW4s"
   },
   "source": [
    "---\n",
    "<font color=green>Q3: (3 Marks)</font>\n",
    "<br><font color='green'>\n",
    "Using **[yfinance](https://pypi.org/project/yfinance/)** library, write a Python script that accepts a list of stock ticker symbols. For each symbol, download the adjusted closing price data, store it in a dictionary with the ticker symbol as the key, and then convert the final dictionary into a Pandas DataFrame. Handle any errors encountered during data retrieval by printing a message indicating which symbol failed\n",
    "</font>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pynohwbpxEg5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ADBE         ADP        ABNB       GOOGL        GOOG  \\\n",
      "Date                                                                     \n",
      "1962-01-02         NaN         NaN         NaN         NaN         NaN   \n",
      "1962-01-03         NaN         NaN         NaN         NaN         NaN   \n",
      "1962-01-04         NaN         NaN         NaN         NaN         NaN   \n",
      "1962-01-05         NaN         NaN         NaN         NaN         NaN   \n",
      "1962-01-08         NaN         NaN         NaN         NaN         NaN   \n",
      "...                ...         ...         ...         ...         ...   \n",
      "2024-05-31  444.760010  244.919998  144.929993  172.500000  173.960007   \n",
      "2024-06-03  439.019989  244.020004  146.250000  173.169998  174.419998   \n",
      "2024-06-04  448.369995  245.669998  147.080002  173.789993  175.130005   \n",
      "2024-06-05  455.799988  245.779999  145.779999  175.410004  177.070007   \n",
      "2024-06-06  464.440002  247.634995  147.639999  176.440598  177.940002   \n",
      "\n",
      "                  AMZN         AMD        AEP        AMGN         ADI  ...  \\\n",
      "Date                                                                   ...   \n",
      "1962-01-02         NaN         NaN   0.950191         NaN         NaN  ...   \n",
      "1962-01-03         NaN         NaN   0.948460         NaN         NaN  ...   \n",
      "1962-01-04         NaN         NaN   0.934614         NaN         NaN  ...   \n",
      "1962-01-05         NaN         NaN   0.913845         NaN         NaN  ...   \n",
      "1962-01-08         NaN         NaN   0.906922         NaN         NaN  ...   \n",
      "...                ...         ...        ...         ...         ...  ...   \n",
      "2024-05-31  176.440002  166.899994  90.250000  305.850006  233.560974  ...   \n",
      "2024-06-03  178.339996  163.550003  90.080002  307.420013  231.290009  ...   \n",
      "2024-06-04  179.339996  159.990005  90.379997  307.369995  230.630005  ...   \n",
      "2024-06-05  181.279999  166.169998  88.949997  307.380005  235.679993  ...   \n",
      "2024-06-06  183.210007  165.889999  89.345001  308.355011  236.119995  ...   \n",
      "\n",
      "                  TSLA         TXN        TTD        VRSK        VRTX  \\\n",
      "Date                                                                    \n",
      "1962-01-02         NaN         NaN        NaN         NaN         NaN   \n",
      "1962-01-03         NaN         NaN        NaN         NaN         NaN   \n",
      "1962-01-04         NaN         NaN        NaN         NaN         NaN   \n",
      "1962-01-05         NaN         NaN        NaN         NaN         NaN   \n",
      "1962-01-08         NaN         NaN        NaN         NaN         NaN   \n",
      "...                ...         ...        ...         ...         ...   \n",
      "2024-05-31  178.080002  195.009995  92.779999  252.779999  455.339996   \n",
      "2024-06-03  176.289993  193.720001  93.110001  253.750000  470.179993   \n",
      "2024-06-04  174.770004  193.300003  94.480003  258.250000  474.950012   \n",
      "2024-06-05  175.000000  196.080002  97.410004  261.279999  483.040009   \n",
      "2024-06-06  176.887604  195.229996  97.185799  260.135010  482.200012   \n",
      "\n",
      "                  WBA    WBD        WDAY        XEL          ZS  \n",
      "Date                                                             \n",
      "1962-01-02        NaN    NaN         NaN        NaN         NaN  \n",
      "1962-01-03        NaN    NaN         NaN        NaN         NaN  \n",
      "1962-01-04        NaN    NaN         NaN        NaN         NaN  \n",
      "1962-01-05        NaN    NaN         NaN        NaN         NaN  \n",
      "1962-01-08        NaN    NaN         NaN        NaN         NaN  \n",
      "...               ...    ...         ...        ...         ...  \n",
      "2024-05-31  16.219999  8.240  211.449997  55.450001  169.960007  \n",
      "2024-06-03  15.920000  8.330  210.830002  55.279999  169.020004  \n",
      "2024-06-04  16.110001  8.240  211.119995  56.029999  169.139999  \n",
      "2024-06-05  15.940000  8.300  212.460007  55.160000  174.570007  \n",
      "2024-06-06  15.815000  8.365  215.720001  54.770000  179.520004  \n",
      "\n",
      "[15715 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q3\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def download_adjusted_close(tickers):\n",
    "    data_dict = {}\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            data = yf.download(ticker, progress=False) \n",
    "            data_dict[ticker] = data['Adj Close']\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to retrieve data for {ticker}: {e}\")\n",
    "    \n",
    "    nasdaq100 = pd.DataFrame(data_dict)\n",
    "    \n",
    "    return nasdaq100\n",
    "\n",
    "tickers = tickers_list\n",
    "nasdaq100 = download_adjusted_close(tickers)\n",
    "\n",
    "print(nasdaq100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np3XHMtatQC_"
   },
   "source": [
    "---\n",
    "<font color=green>Q4: (3 Marks)</font>\n",
    "<br><font color='green'>\n",
    "Write a Python script to analyze stock data stored in a dictionary `stock_data` (where each key is a stock ticker symbol, and each value is a Pandas Series of adjusted closing prices). The script should:\n",
    "1. Convert the dictionary into a DataFrame.\n",
    "2. Calculate the daily returns for each stock.\n",
    "3. Identify columns (ticker symbols) with at least 2000 non-NaN values in their daily returns.\n",
    "4. Create a new DataFrame that only includes these filtered ticker symbols.\n",
    "5. Remove any remaining rows with NaN values in this new DataFrame.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2pt3y9_IxFjz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
      "Date                                                                     \n",
      "2015-12-10 -0.006699  0.006004 -0.003292 -0.002861 -0.003715  0.042553   \n",
      "2015-12-11  0.027653 -0.024924 -0.012657 -0.014130 -0.033473 -0.036735   \n",
      "2015-12-14  0.020127  0.015240  0.016151  0.012045  0.027744 -0.008475   \n",
      "2015-12-15  0.008149  0.010284 -0.003213 -0.005844  0.001110  0.008547   \n",
      "2015-12-16  0.016380  0.008541  0.021708  0.019761  0.026008  0.076271   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-05-31 -0.002489  0.016645  0.002266  0.002305 -0.016061  0.000900   \n",
      "2024-06-03 -0.012906 -0.003675  0.003884  0.002644  0.010768 -0.020072   \n",
      "2024-06-04  0.021297  0.006762  0.003580  0.004071  0.005607 -0.021767   \n",
      "2024-06-05  0.016571  0.000448  0.009322  0.011077  0.010817  0.038627   \n",
      "2024-06-06  0.018956  0.007547  0.005875  0.004913  0.010647 -0.001685   \n",
      "\n",
      "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
      "Date                                                ...                       \n",
      "2015-12-10 -0.024356  0.011274  0.008826  0.004968  ... -0.004777  0.007485   \n",
      "2015-12-11 -0.005831 -0.028308 -0.003849 -0.013951  ... -0.011575 -0.009356   \n",
      "2015-12-14 -0.000367  0.019078 -0.001932  0.003899  ...  0.005141  0.014444   \n",
      "2015-12-15  0.027503  0.028524 -0.004752  0.009544  ...  0.023018  0.044907   \n",
      "2015-12-16  0.017309  0.012053  0.017330  0.008354  ...  0.006389  0.025419   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "2024-05-31  0.023707  0.016113  0.019522 -0.007690  ...  0.003630  0.028874   \n",
      "2024-06-03 -0.001884  0.005133 -0.009723 -0.008505  ...  0.020454 -0.010745   \n",
      "2024-06-04  0.003330 -0.000163 -0.002854  0.007466  ...  0.014178  0.027906   \n",
      "2024-06-05 -0.015822  0.000033  0.021896  0.031441  ...  0.008857  0.013265   \n",
      "2024-06-06  0.004441  0.003172  0.001867 -0.009631  ... -0.005555  0.001803   \n",
      "\n",
      "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
      "Date                                                                     \n",
      "2015-12-10  0.011358  0.002995 -0.002615  0.010279  0.000960 -0.002109   \n",
      "2015-12-11 -0.044259 -0.012647 -0.015996 -0.034709 -0.020499 -0.035928   \n",
      "2015-12-14  0.007188  0.000178  0.014390 -0.016491  0.010403 -0.033613   \n",
      "2015-12-15  0.011483  0.023657  0.013924  0.013656 -0.005450  0.002646   \n",
      "2015-12-16  0.060699  0.009036  0.021117  0.010658  0.031665  0.024887   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-05-31 -0.003971 -0.003424  0.013674  0.027740  0.053931  0.019802   \n",
      "2024-06-03 -0.010052 -0.006615  0.003837  0.032591 -0.018496  0.010922   \n",
      "2024-06-04 -0.008622 -0.002168  0.017734  0.010145  0.011935 -0.010804   \n",
      "2024-06-05  0.001316  0.014382  0.011733  0.017033 -0.010553  0.007282   \n",
      "2024-06-06  0.010786 -0.004335 -0.004382 -0.001739 -0.007842  0.007831   \n",
      "\n",
      "                WDAY       XEL  \n",
      "Date                            \n",
      "2015-12-10  0.005037 -0.013889  \n",
      "2015-12-11 -0.057630  0.004311  \n",
      "2015-12-14  0.000253  0.010017  \n",
      "2015-12-15 -0.000380  0.007934  \n",
      "2015-12-16  0.029378  0.023896  \n",
      "...              ...       ...  \n",
      "2024-05-31  0.019331  0.020803  \n",
      "2024-06-03 -0.002932 -0.003066  \n",
      "2024-06-04  0.001375  0.013567  \n",
      "2024-06-05  0.006347 -0.015527  \n",
      "2024-06-06  0.015344 -0.007070  \n",
      "\n",
      "[2136 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q4\n",
    "\n",
    "# Find the daily percentage change\n",
    "stock_data = nasdaq100.pct_change()\n",
    "#Filter stocks that have more than 2000 NA daily values\n",
    "valid_columns = stock_data.columns[stock_data.count() >= 2000]\n",
    "filtered_nasdaq100 = stock_data[valid_columns]\n",
    "#Drop rows containing NA values\n",
    "filtered_nasdaq100 = filtered_nasdaq100.dropna()\n",
    "\n",
    "#Print the filtered DataFrame\n",
    "print(filtered_nasdaq100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3xAawJ8nPbH"
   },
   "source": [
    "---\n",
    "<font color=green>Q5: (1 Mark)</font>\n",
    "<br><font color='green'>\n",
    "Download the dataset named `df_filtered_nasdaq_100` from the GitHub repository of the course.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ko4juu_HxHnT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
      "Date                                                                     \n",
      "2015-12-10 -0.006699  0.006004 -0.003292 -0.002861 -0.003715  0.042553   \n",
      "2015-12-11  0.027653 -0.024924 -0.012657 -0.014130 -0.033473 -0.036735   \n",
      "2015-12-14  0.020127  0.015241  0.016151  0.012045  0.027744 -0.008475   \n",
      "2015-12-15  0.008149  0.010283 -0.003213 -0.005844  0.001110  0.008547   \n",
      "2015-12-16  0.016380  0.008541  0.021708  0.019761  0.026008  0.076271   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-05-06  0.015241  0.003514  0.005142  0.004971  0.013372  0.034396   \n",
      "2024-05-07 -0.002674  0.009805  0.018739  0.018548  0.000318 -0.008666   \n",
      "2024-05-08 -0.008471 -0.008894 -0.010920 -0.010521 -0.004026 -0.005245   \n",
      "2024-05-09 -0.011166  0.009097  0.003424  0.002454  0.007979 -0.008007   \n",
      "2024-05-10 -0.000746  0.006975 -0.007708 -0.007518 -0.010660 -0.003084   \n",
      "\n",
      "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
      "Date                                                ...                       \n",
      "2015-12-10 -0.024356  0.011274  0.008826  0.004968  ... -0.004777  0.007485   \n",
      "2015-12-11 -0.005831 -0.028308 -0.003850 -0.013951  ... -0.011575 -0.009356   \n",
      "2015-12-14 -0.000366  0.019078 -0.001932  0.003899  ...  0.005141  0.014444   \n",
      "2015-12-15  0.027503  0.028525 -0.004752  0.009544  ...  0.023018  0.044907   \n",
      "2015-12-16  0.017309  0.012053  0.017330  0.008354  ...  0.006389  0.025419   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "2024-05-06  0.002370 -0.037939  0.018484  0.006478  ...  0.016863 -0.013548   \n",
      "2024-05-07  0.011936  0.002738  0.001230  0.010728  ... -0.000067 -0.001109   \n",
      "2024-05-08  0.007900  0.023343  0.006337  0.005907  ... -0.015910  0.003946   \n",
      "2024-05-09  0.004085  0.018060 -0.000342  0.000887  ... -0.001987  0.011361   \n",
      "2024-05-10  0.007257 -0.008662  0.011719  0.003056  ...  0.001373 -0.002915   \n",
      "\n",
      "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
      "Date                                                                     \n",
      "2015-12-10  0.011358  0.002996 -0.002616  0.010279  0.000960 -0.002109   \n",
      "2015-12-11 -0.044259 -0.012648 -0.015996 -0.034709 -0.020499 -0.035928   \n",
      "2015-12-14  0.007188  0.000178  0.014390 -0.016491  0.010402 -0.033613   \n",
      "2015-12-15  0.011483  0.023657  0.013923  0.013656 -0.005450  0.002646   \n",
      "2015-12-16  0.060699  0.009036  0.021117  0.010658  0.031664  0.024887   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-05-06  0.019703  0.015427  0.019087  0.003540 -0.030881 -0.001255   \n",
      "2024-05-07 -0.037616  0.012752  0.021252  0.019230  0.005214 -0.023869   \n",
      "2024-05-08 -0.017378  0.007007 -0.009838  0.020915 -0.006916  0.003861   \n",
      "2024-05-09 -0.015739  0.007448  0.001676  0.000406  0.001161  0.030769   \n",
      "2024-05-10 -0.020352  0.009335  0.013593  0.009046 -0.003478  0.013682   \n",
      "\n",
      "                WDAY       XEL  \n",
      "Date                            \n",
      "2015-12-10  0.005037 -0.013889  \n",
      "2015-12-11 -0.057630  0.004311  \n",
      "2015-12-14  0.000253  0.010017  \n",
      "2015-12-15 -0.000380  0.007934  \n",
      "2015-12-16  0.029378  0.023897  \n",
      "...              ...       ...  \n",
      "2024-05-06 -0.022949  0.002028  \n",
      "2024-05-07 -0.001921  0.012141  \n",
      "2024-05-08  0.000802 -0.001636  \n",
      "2024-05-09 -0.014702  0.005644  \n",
      "2024-05-10  0.001545  0.003983  \n",
      "\n",
      "[2118 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q5\n",
    "\n",
    "#Get file from github page\n",
    "df_filtered_nasdaq_100 = pd.read_csv(\"https://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/df_filtered_nasdaq_100.csv\",index_col=0, parse_dates=True)\n",
    "\n",
    "print(df_filtered_nasdaq_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WbCUDabWQoZ"
   },
   "source": [
    "---\n",
    "<font color=green>Q6: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Conduct an in-depth analysis of the `df_filtered_nasdaq_100` dataset from GitHub. Answer the following questions:\n",
    "- Which stock had the best performance over the entire period?\n",
    "- What is the average daily return of 'AAPL'?\n",
    "- What is the worst daily return? Provide the stock name and the date it occurred.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kPfiDWWlxI0Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Best performing stock over the entire period: NVDA\n",
      "2. Average daily return of 'AAPL': 0.0010849409515136207\n",
      "3. Worst daily return occurred on 2020-03-09 00:00:00 for the stock FANG with a return of -0.4464579394678258\n"
     ]
    }
   ],
   "source": [
    "#Q6\n",
    "\n",
    "# Calculate cumulative returns for each stock\n",
    "cumulative_returns = (1 + df_filtered_nasdaq_100).cumprod() \n",
    "\n",
    "# Identify the stock with the highest cumulative return\n",
    "best_performing_stock = cumulative_returns.iloc[-1].idxmax()\n",
    "\n",
    "# Calculate the average daily return of 'AAPL'\n",
    "average_daily_return_aapl = df_filtered_nasdaq_100['AAPL'].mean()\n",
    "\n",
    "# Find the worst daily return and identify the stock name and the date it occurred\n",
    "daily_returns = df_filtered_nasdaq_100\n",
    "worst_daily_return_stock = daily_returns.min().idxmin()\n",
    "worst_daily_return_value = daily_returns[worst_daily_return_stock].min()\n",
    "worst_daily_return_date = daily_returns[worst_daily_return_stock].idxmin()\n",
    "\n",
    "# Print the results\n",
    "print(\"1. Best performing stock over the entire period:\", best_performing_stock)\n",
    "print(\"2. Average daily return of 'AAPL':\", average_daily_return_aapl)\n",
    "print(\"3. Worst daily return occurred on\", worst_daily_return_date, \"for the stock\", worst_daily_return_stock, \"with a return of\", worst_daily_return_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcY-V82zXGqc"
   },
   "source": [
    "# Fama French Analysis\n",
    "\n",
    "The Fama-French five-factor model is an extension of the classic three-factor model used in finance to describe stock returns. It is designed to better capture the risk associated with stocks and explain differences in returns. This model includes the following factors:\n",
    "\n",
    "1. **Market Risk (MKT)**: The excess return of the market over the risk-free rate. It captures the overall market's premium.\n",
    "2. **Size (SMB, \"Small Minus Big\")**: The performance of small-cap stocks relative to large-cap stocks.\n",
    "3. **Value (HML, \"High Minus Low\")**: The performance of stocks with high book-to-market values relative to those with low book-to-market values.\n",
    "4. **Profitability (RMW, \"Robust Minus Weak\")**: The difference in returns between companies with robust (high) and weak (low) profitability.\n",
    "5. **Investment (CMA, \"Conservative Minus Aggressive\")**: The difference in returns between companies that invest conservatively and those that invest aggressively.\n",
    "\n",
    "## Additional Factor\n",
    "\n",
    "6. **Momentum (MOM)**: This factor represents the tendency of stocks that have performed well in the past to continue performing well, and the reverse for stocks that have performed poorly.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "The return of a stock $R_i^t$ at time $t$ can be modeled as follows :\n",
    "\n",
    "$$\n",
    "R_i^t - R_f^t = \\alpha_i^t + \\beta_{i,MKT}^t(R_M^t - R_f^t) + \\beta_{i,SMB}^t \\cdot SMB^t + \\beta_{i,HML}^t \\cdot HML^t + \\beta_{i,RMW}^t \\cdot RMW^t + \\beta_{i,CMA}^t \\cdot CMA^t + \\beta_{i,MOM}^t \\cdot MOM^t + \\epsilon_i^t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ R_i^t $ is the return of stock $i$ at time $t$\n",
    "- $R_f^t $is the risk-free rate at time $t$\n",
    "- $ R_M^t $ is the market return at time $t$\n",
    "- $\\alpha_i^t $ is the abnormal return or alpha of stock $ i $ at time $t$\n",
    "- $\\beta^t $ coefficients represent the sensitivity of the stock returns to each factor at time $t$\n",
    "- $\\epsilon_i^t $ is the error term or idiosyncratic risk unique to stock $ i $ at time $t$\n",
    "\n",
    "This model is particularly useful for identifying which factors significantly impact stock returns and for constructing a diversified portfolio that is optimized for given risk preferences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtFgmpsKQc8B"
   },
   "source": [
    "---\n",
    "<font color=green>Q7: (1 Mark) </font>\n",
    "<br><font color='green'>\n",
    "Download the `fama_french_dataset` from the course's GitHub account.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iJVPHhTSxKuk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Mkt-RF   SMB   HML   RMW   CMA     RF   Mom\n",
      "Date                                                   \n",
      "1963-07-01   -0.67  0.02 -0.35  0.03  0.13  0.012 -0.21\n",
      "1963-07-02    0.79 -0.28  0.28 -0.08 -0.21  0.012  0.42\n",
      "1963-07-03    0.63 -0.18 -0.10  0.13 -0.25  0.012  0.41\n",
      "1963-07-05    0.40  0.09 -0.28  0.07 -0.30  0.012  0.07\n",
      "1963-07-08   -0.63  0.07 -0.20 -0.27  0.06  0.012 -0.45\n",
      "...            ...   ...   ...   ...   ...    ...   ...\n",
      "2024-03-22   -0.23 -0.98 -0.53  0.29 -0.37  0.021  0.43\n",
      "2024-03-25   -0.26 -0.10  0.88 -0.22 -0.17  0.021 -0.34\n",
      "2024-03-26   -0.26  0.10 -0.13 -0.50  0.23  0.021  0.09\n",
      "2024-03-27    0.88  1.29  0.91 -0.14  0.58  0.021 -1.34\n",
      "2024-03-28    0.10  0.45  0.48 -0.07  0.09  0.021 -0.44\n",
      "\n",
      "[15290 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q7\n",
    "\n",
    "#Get the data from the github page\n",
    "fama_french_data = pd.read_csv(\"https://github.com/Jandsy/ml_finance_imperial/raw/main/Coursework/fama_french_dataset.csv\")\n",
    "\n",
    "# Rename the first column to 'Date'\n",
    "fama_french_data = fama_french_data.rename(columns={fama_french_data.columns[0]: 'Date'})\n",
    "\n",
    "# Convert 'Date' column to datetime\n",
    "fama_french_data['Date'] = pd.to_datetime(fama_french_data['Date'])\n",
    "\n",
    "\"\"\"Filter data between the dates we want. If we want to filter by the papers' dates we can change the values accordingly\"\"\"  \n",
    "start_date = '1963-01-07'\n",
    "end_date = '2024-03-28'\n",
    "filtered_date = (fama_french_data['Date'] >= start_date) & (fama_french_data['Date'] <= end_date)\n",
    "fama_french_data = fama_french_data.loc[filtered_date]\n",
    "\n",
    "# Set 'Date' as the index\n",
    "fama_french_data.set_index('Date', inplace=True)\n",
    "\n",
    "print(fama_french_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5FJ362KW-wh"
   },
   "source": [
    "---\n",
    "<font color=green>Q8: (5 Marks)</font>\n",
    "<br><font color='green'>\n",
    "\n",
    "Write a Python function called `get_sub_df_ticker(ticker, date, df_filtered, length_history)` that extracts a historical sub-dataframe for a given `ticker` from `df_filtered`. The function should use `length_history` to determine the number of trading days to include, ending at the specified `date`. Return the sub-dataframe for the specified `ticker`.\n",
    "</font>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2mIrS_fpxLsR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 AMD\n",
      "Date                \n",
      "2020-01-06 -0.004321\n",
      "2020-01-07 -0.002893\n",
      "2020-01-08 -0.008705\n",
      "2020-01-09  0.023834\n",
      "2020-01-10 -0.016337\n",
      "...              ...\n",
      "2020-12-28 -0.002287\n",
      "2020-12-29 -0.010699\n",
      "2020-12-30  0.018429\n",
      "2020-12-31 -0.006285\n",
      "2021-01-04  0.006433\n",
      "\n",
      "[252 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q8\n",
    "\n",
    "def get_sub_df_ticker(ticker, date, df_filtered_nasdaq_100, length_history):\n",
    "    \n",
    "    date = pd.to_datetime(date)\n",
    "    \n",
    "    ticker_data = df_filtered_nasdaq_100[[ticker]]\n",
    "\n",
    "    end_index = ticker_data.index.get_loc(date)\n",
    "\n",
    "    start_index = max(0, end_index - length_history + 1)\n",
    "   \n",
    "    sub_df = ticker_data.iloc[start_index:end_index + 1]\n",
    "    \n",
    "    return sub_df\n",
    "\n",
    "#Example of function usage:\n",
    "ticker = 'AMD'\n",
    "date = '2021-01-04'\n",
    "length_history =252\n",
    "\n",
    "sub_df = get_sub_df_ticker(ticker, date, df_filtered_nasdaq_100, length_history)\n",
    "\n",
    "#Display the DataFrame\n",
    "print(sub_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-p7nCNYXYNr"
   },
   "source": [
    "---\n",
    "<font color=green>Q9: (4 Marks)</font>\n",
    "<br><font color='green'>\n",
    "Create a Python function named `df_ticker_with_fama_french(ticker, date, df_filtered, length_history, fama_french_data)` that uses `get_sub_df_ticker` to extract historical data for a specific `ticker`. Incorporate the Fama-French factors from `fama_french_data` into the extracted sub-dataframe. Adjust the ticker's returns by subtracting the risk-free rate ('RF') and add other relevant Fama-French factors ('Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', and 'Mom'). Return the resulting sub-dataframe.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pfsdj79HxMnp",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 AMD  Mkt-RF   SMB   HML   RMW   CMA     RF   Mom  \\\n",
      "Date                                                                \n",
      "2020-01-06 -0.004321    0.36 -0.20 -0.55 -0.17 -0.26  0.006 -0.69   \n",
      "2020-01-07 -0.002893   -0.19 -0.03 -0.25 -0.12 -0.25  0.006  0.01   \n",
      "2020-01-08 -0.008705    0.47 -0.17 -0.64 -0.19 -0.17  0.006  0.92   \n",
      "2020-01-09  0.023834    0.65 -0.71 -0.48 -0.14  0.04  0.006  0.73   \n",
      "2020-01-10 -0.016337   -0.34 -0.27 -0.33  0.04 -0.08  0.006  0.18   \n",
      "...              ...     ...   ...   ...   ...   ...    ...   ...   \n",
      "2020-12-28 -0.002287    0.46 -0.68  0.36  1.39  0.46  0.000 -0.47   \n",
      "2020-12-29 -0.010699   -0.40 -1.42  0.24  0.78 -0.29  0.000 -0.41   \n",
      "2020-12-30  0.018429    0.27  1.06  0.03 -0.65 -0.05  0.000 -0.27   \n",
      "2020-12-31 -0.006285    0.39 -0.71  0.41  0.57 -0.24  0.000 -0.59   \n",
      "2021-01-04  0.006433   -1.41  0.16  0.58 -0.64  0.10  0.000 -0.04   \n",
      "\n",
      "            Excess Return  \n",
      "Date                       \n",
      "2020-01-06      -0.010321  \n",
      "2020-01-07      -0.008893  \n",
      "2020-01-08      -0.014705  \n",
      "2020-01-09       0.017834  \n",
      "2020-01-10      -0.022337  \n",
      "...                   ...  \n",
      "2020-12-28      -0.002287  \n",
      "2020-12-29      -0.010699  \n",
      "2020-12-30       0.018429  \n",
      "2020-12-31      -0.006285  \n",
      "2021-01-04       0.006433  \n",
      "\n",
      "[252 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q9 \n",
    "\n",
    "def df_ticker_with_fama_french(ticker, date, df_filtered_nasdaq_100, length_history, fama_french_data):\n",
    "    \n",
    "    sub_df = get_sub_df_ticker(ticker, date, df_filtered_nasdaq_100, length_history)\n",
    "    \n",
    "    # Join stock data with Fama-French factors\n",
    "    combined_df = sub_df.join(fama_french_data, how='inner') \n",
    "    \n",
    "    # Calculate Excess Return\n",
    "    combined_df['Excess Return'] = combined_df[ticker] - combined_df['RF']\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "combined_df = df_ticker_with_fama_french(ticker, date, df_filtered_nasdaq_100, length_history, fama_french_data)\n",
    "\n",
    "#Display the DataFrame\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykVqmW4PQe5T"
   },
   "source": [
    "---\n",
    "<font color=green>Q10: (5 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Write a Python function named `extract_beta_fama_french` to perform a rolling regression analysis for a given stock at a specific time point using the Fama-French model. The function should accept the following parameters:\n",
    "\n",
    "- `ticker`: A string indicating the stock symbol.\n",
    "- `date`: A string specifying the date for the analysis.\n",
    "- `length_history`: An integer representing the number of days of historical data to include.\n",
    "- `df_filtered`: A pandas DataFrame (assumed to be derived from question 5) containing filtered stock data.\n",
    "- `fama_french_data`: A pandas DataFrame (assumed to be from question 7) that includes Fama-French factors.\n",
    "\n",
    "Utilize the `statsmodels.api` library to conduct the regression.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def extract_beta_fama_french(ticker, date, length_history, df_filtered_nasdaq_100, fama_french_data):\n",
    "    \n",
    "    # Ensure the date is in datetime format\n",
    "    date = pd.to_datetime(date)\n",
    "    \n",
    "    # Get function from previous question and specify window size\n",
    "    combined_df = df_ticker_with_fama_french(ticker, date, df_filtered_nasdaq_100, length_history, fama_french_data)\n",
    "    window_size = 60\n",
    "\n",
    "    rolling_betas = []\n",
    "    rolling_residuals = []\n",
    "\n",
    "    dates = combined_df.index[window_size:]\n",
    "    \n",
    "    # This is used to find R Squared of the model needed for question 19\n",
    "    last_model = None\n",
    "\n",
    "\n",
    "    for ending_date in dates:\n",
    "        try:\n",
    "            #Starting date for rolling window\n",
    "            starting_date = ending_date - pd.DateOffset(days=window_size)\n",
    "            #Use combined dataframe to extract relevant data to window df\n",
    "            window_df = combined_df.loc[starting_date:ending_date]\n",
    "            y = window_df['Excess Return']\n",
    "            X = window_df[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'Mom']]\n",
    "            X = sm.add_constant(X)\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            #Extract betas and residuals\n",
    "            betas = model.params\n",
    "            residuals = model.resid\n",
    "            betas['Date'] = ending_date\n",
    "            rolling_betas.append(betas)\n",
    "            \n",
    "            \n",
    "            last_model = model\n",
    "\n",
    "            #Store the mean residual for the current window's ending_date\n",
    "            rolling_residuals.append({'Date': ending_date, 'Residual': residuals.mean()})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to run regression for {ending_date}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Combine betas into a DataFrame\n",
    "    betas_df = pd.DataFrame(rolling_betas).set_index('Date')\n",
    "\n",
    "    # Combine residuals into a DataFrame\n",
    "    residuals_df = pd.DataFrame(rolling_residuals).set_index('Date')\n",
    "    \n",
    "    #Calculate R^2\n",
    "    r_squared = last_model.rsquared if last_model else None\n",
    "\n",
    "    return betas_df, residuals_df, model.summary(),r_squared\n",
    "\n",
    "ff_results = extract_beta_fama_french(ticker, date, length_history, df_filtered_nasdaq_100, fama_french_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BY8FqrBjRDTz"
   },
   "source": [
    "---\n",
    "<font color=green>Q11: (2 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Apply the `extract_beta_fama_french` function to the stock symbol 'AAPL' for the date '2024-03-28', using a historical data length of 252 days. Ensure that the `df_filtered` and `fama_french_data` DataFrames are correctly prepared and available in your environment before executing this function. The parameters for the function call are set as follows:\n",
    "\n",
    "- **Ticker**: 'AAPL'\n",
    "- **Date**: '2024-03-28'\n",
    "- **Length of History**: 252 days\n",
    "</font>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fo6C5-qXxPZO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               const    Mkt-RF       SMB       HML       RMW       CMA  \\\n",
      "Date                                                                     \n",
      "2023-06-26 -0.016966  0.010593 -0.001548 -0.003647  0.004572 -0.001838   \n",
      "2023-06-27 -0.017074  0.010140 -0.001437 -0.003380  0.004252 -0.002091   \n",
      "2023-06-28 -0.016942  0.010157 -0.001213 -0.003245  0.003928 -0.002249   \n",
      "2023-06-29 -0.017018  0.010088 -0.001325 -0.003162  0.003801 -0.002438   \n",
      "2023-06-30 -0.016837  0.010572 -0.001556 -0.003048  0.003414 -0.002081   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-03-22 -0.024051  0.006684  0.002445 -0.011018  0.005096  0.003073   \n",
      "2024-03-25 -0.024377  0.006824  0.000997 -0.010806  0.004160  0.002315   \n",
      "2024-03-26 -0.024405  0.006677  0.001091 -0.010902  0.004264  0.001898   \n",
      "2024-03-27 -0.023653  0.007882  0.001642 -0.011233  0.005588  0.002879   \n",
      "2024-03-28 -0.023836  0.007968  0.001566 -0.011303  0.005548  0.003230   \n",
      "\n",
      "                 Mom  \n",
      "Date                  \n",
      "2023-06-26 -0.001153  \n",
      "2023-06-27 -0.000943  \n",
      "2023-06-28 -0.000614  \n",
      "2023-06-29 -0.000614  \n",
      "2023-06-30 -0.000297  \n",
      "...              ...  \n",
      "2024-03-22 -0.003848  \n",
      "2024-03-25 -0.004941  \n",
      "2024-03-26 -0.005058  \n",
      "2024-03-27 -0.005985  \n",
      "2024-03-28 -0.005808  \n",
      "\n",
      "[192 rows x 7 columns]\n",
      "                Residual\n",
      "Date                    \n",
      "2023-06-26 -1.083144e-17\n",
      "2023-06-27 -2.887680e-18\n",
      "2023-06-28  6.431170e-18\n",
      "2023-06-29 -4.832444e-18\n",
      "2023-06-30  9.561150e-18\n",
      "...                  ...\n",
      "2024-03-22 -7.885107e-19\n",
      "2024-03-25  4.956353e-19\n",
      "2024-03-26 -8.756223e-18\n",
      "2024-03-27  5.865017e-18\n",
      "2024-03-28 -1.678244e-17\n",
      "\n",
      "[192 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q11\n",
    "\n",
    "ticker = 'AAPL'\n",
    "date = '2024-03-28'\n",
    "length_history = 252\n",
    "\n",
    "#Call the function we created before\n",
    "ff_results_apple = extract_beta_fama_french(ticker, date, length_history, df_filtered_nasdaq_100, fama_french_data)\n",
    "\n",
    "#Print the results for AAPL\n",
    "print(ff_results_apple[0])  # Betas DataFrame\n",
    "print(ff_results_apple[1])  # Residuals DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DyA4G1d0HjY"
   },
   "source": [
    "---\n",
    "<font color=green>Q12: (2 Marks)</font>\n",
    "<br><font color='green'>\n",
    "Once the `extract_beta_fama_french` function has been applied to 'AAPL' with the specified parameters, the next step is to analyze the regression summary to identify which Fama-French factor explains the most variance in 'AAPL' returns during the specified period.\n",
    "\n",
    "Follow these steps to perform the analysis:\n",
    "\n",
    "1. **Review the Summary**: Examine the regression output, focusing on the coefficients and their statistical significance (p-values).\n",
    "2. **Identify Key Factor**: Determine which factor has the highest absolute coefficient value and is statistically significant (typically p < 0.05). This factor can be considered as having the strongest influence on 'AAPL' returns for the period.\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          Excess Return   R-squared:                       0.385\n",
      "Model:                            OLS   Adj. R-squared:                  0.283\n",
      "Method:                 Least Squares   F-statistic:                     3.760\n",
      "Date:                Thu, 06 Jun 2024   Prob (F-statistic):            0.00524\n",
      "Time:                        17:50:55   Log-Likelihood:                 138.32\n",
      "No. Observations:                  43   AIC:                            -262.6\n",
      "Df Residuals:                      36   BIC:                            -250.3\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0238      0.002    -13.392      0.000      -0.027      -0.020\n",
      "Mkt-RF         0.0080      0.003      2.357      0.024       0.001       0.015\n",
      "SMB            0.0016      0.003      0.508      0.614      -0.005       0.008\n",
      "HML           -0.0113      0.003     -3.279      0.002      -0.018      -0.004\n",
      "RMW            0.0055      0.004      1.397      0.171      -0.003       0.014\n",
      "CMA            0.0032      0.007      0.450      0.655      -0.011       0.018\n",
      "Mom           -0.0058      0.004     -1.556      0.128      -0.013       0.002\n",
      "==============================================================================\n",
      "Omnibus:                        2.698   Durbin-Watson:                   2.086\n",
      "Prob(Omnibus):                  0.260   Jarque-Bera (JB):                1.831\n",
      "Skew:                          -0.143   Prob(JB):                        0.400\n",
      "Kurtosis:                       3.970   Cond. No.                         6.45\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Q12\n",
    "\n",
    "ff_results_apple = extract_beta_fama_french(ticker, date, length_history, df_filtered_nasdaq_100, fama_french_data)\n",
    "\n",
    "#Print OLS Regression Results\n",
    "print(ff_results_apple[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thaIsaiFWWNO"
   },
   "source": [
    "The factor that is statistically significant (P-Value < 0.05) and with the highest coefficient value is the Mkt-RF factor, representing the excess market return over the risk free rate. This is the factor with the highest influence for AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pj7qlAq-J8N2"
   },
   "source": [
    "# PCA Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qty6V05YXxti"
   },
   "source": [
    "\n",
    "In literature, another method exists for extracting residuals for each stock, utilizing the PCA approach to identify hidden factors in the data. Let's describe this method.\n",
    "\n",
    "The return of a stock $R_i^t$ at time $t$ can be modeled as follows :\n",
    "\n",
    "$$\n",
    "R_i^t  = \\sum_{j=1}^m\\beta_{i,j}^t F_j^t  + \\epsilon_i^t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ R_i^t $ is the return of stock $i$ at time $t$\n",
    "- $m$ is the number of factors selected from PCA\n",
    "-  $ F_j^t $ is the $j$-th hidden factor constructed from PCA at time $t$\n",
    "- $\\beta_{i,j}^t $ are the coefficients representing the sensitivity of the stock returns to each hidden factor.\n",
    "- $\\epsilon_i^t $  is the residual term for stock $i$ at time $t$, representing the portion of the return not explained by the PCA factors.\n",
    "\n",
    "### Representation of Stock Return Data\n",
    "\n",
    "Consider the return data for $N$ stocks over $T$ periods, represented by the matrix $R$ of size $T \\times N$:\n",
    "\n",
    "$$\n",
    "R = \\left[\n",
    "\\begin{array}{cccc}\n",
    "R_1^T & R_2^T & \\cdots & R_N^T \\\\\n",
    "R_1^{T-1} & R_2^{T-1} & \\cdots & R_N^{T-1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "R_1^1 & R_2^1 & \\cdots & R_N^1 \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Each element $R_i^k$ of the matrix represents the return of stock $i$ at time $k$ and is defined as:\n",
    "\n",
    "$$\n",
    "R_i^k = \\frac{S_{i,k} - S_{i, k-1}}{S_{i, k-1}}, \\quad k=1,\\cdots, T, \\quad i=1,\\cdots,N\n",
    "$$\n",
    "\n",
    "where $S_{i,k}$ denotes the adjusted close price of stock $i$ at time $k$.\n",
    "\n",
    "### Standardization of Returns\n",
    "\n",
    "To adjust for varying volatilities across stocks, we standardize the returns as follows:\n",
    "\n",
    "$$\n",
    "Z_i^t = \\frac{R_i^t - \\mu_i}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "where $\\mu_i$ and $\\sigma_i$ are the mean and standard deviation of returns for stock $i$ over the period $[t-T, t]$, respectively.\n",
    "\n",
    "### Empirical Correlation Matrix\n",
    "\n",
    "The empirical correlation matrix $C$ is computed from the standardized returns:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{T-1} Z^T Z\n",
    "$$\n",
    "\n",
    "where $Z^T$ is the transpose of matrix $Z$.\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "We apply Singular Value Decomposition to the correlation matrix $C$:\n",
    "\n",
    "$$\n",
    "C = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "Here, $U$ and $V$ are orthogonal matrices representing the left and right singular vectors, respectively, and $\\Sigma$ is a diagonal matrix containing the singular values, which are the square roots of the eigenvalues.\n",
    "\n",
    "### Construction of Hidden Factors\n",
    "\n",
    "For each of the top $m$ components, we construct the selected hidden factors as follows:\n",
    "\n",
    "$$\n",
    "F_j^t = \\sum_{i=1}^N \\frac{\\lambda_{i,j}}{\\sigma_i} R_i^t\n",
    "$$\n",
    "\n",
    "where $\\lambda_{i,j}$ is the $i$-th component of the $j$-th eigenvector (ranked by eigenvalue magnitude).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIkNjUbFZ3Wy"
   },
   "source": [
    "---\n",
    "<font color=green>Q13 (3 Marks):\n",
    "\n",
    "For the specified period from March 29, 2023 ('2023-03-29'), to March 28, 2024 ('2024-03-28'), generate the matrix $Z$ by standardizing the stock returns using the DataFrame `df_filtered_new`\n",
    "</font>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AqhmrY9xcbnk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
      "Date                                                                     \n",
      "2023-03-29  0.658925  2.189521  0.106285  0.207821  1.503570  0.444433   \n",
      "2023-03-30  0.273051 -0.221306 -0.389329 -0.434766  0.787034  0.526996   \n",
      "2023-03-31  0.360570  1.136308  1.540732  1.439604  0.531735 -0.056439   \n",
      "2023-04-03 -0.713053 -2.259591  0.252735  0.407399 -0.591892 -0.600163   \n",
      "2023-04-04  0.560730 -1.137430  0.099652  0.013877  0.658632 -0.342217   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-03-22 -1.146803 -0.516681  1.151431  1.085070  0.074915  0.081847   \n",
      "2024-03-25  0.659349 -1.221008 -0.372489 -0.341074  0.109668 -0.292706   \n",
      "2024-03-26 -0.032708  0.234343  0.131651  0.109342 -0.556130 -0.244717   \n",
      "2024-03-27 -0.363721  1.052056 -0.024166 -0.010591  0.315892  0.224881   \n",
      "2024-03-28 -0.048375  0.411934 -0.078409  0.019962  0.022729  0.067776   \n",
      "\n",
      "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
      "Date                                                ...                       \n",
      "2023-03-29  1.020467  0.727398  1.860839  0.356414  ...  0.517876  0.615242   \n",
      "2023-03-30  0.277291  0.076716  1.630406  0.936497  ... -0.109884  0.439972   \n",
      "2023-03-31  0.475365  0.008639  0.935425  1.044069  ...  1.337544  0.117646   \n",
      "2023-04-03 -0.086824  0.759731 -0.328378 -0.555499  ... -0.377687  1.191774   \n",
      "2023-04-04  0.223111  0.872403 -0.399698 -0.127907  ...  1.434972 -0.347689   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "2024-03-22 -0.150701 -0.278013 -0.555224  0.126867  ...  0.046870 -0.246035   \n",
      "2024-03-25 -0.084892  1.184710 -0.959285 -0.282029  ... -2.575803  0.240999   \n",
      "2024-03-26 -0.377797  0.183364 -0.577464  0.317533  ...  0.150958 -0.070195   \n",
      "2024-03-27  2.192442  1.128111  1.411127 -0.309662  ...  0.034693  0.474269   \n",
      "2024-03-28  1.190631 -0.583106  1.407557 -0.141870  ...  0.577964  0.645940   \n",
      "\n",
      "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
      "Date                                                                     \n",
      "2023-03-29  0.814986  1.344650  1.105776  0.127319  0.497115  0.429736   \n",
      "2023-03-30  0.233493  1.187056  0.239468 -0.525980  0.691243  0.446100   \n",
      "2023-03-31  2.058867  0.640237  0.325258  0.538943 -0.008816  0.565842   \n",
      "2023-04-03 -2.030038 -0.684867 -0.214460  0.183344  1.205798 -0.547975   \n",
      "2023-04-04 -0.377655 -1.394525 -0.538698 -0.487155  0.553153  0.755053   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-03-22 -0.386620 -0.054031 -0.476030 -0.091837 -0.421327 -0.946797   \n",
      "2024-03-25  0.343241 -0.651297 -1.151639 -0.024341  0.166127  0.118796   \n",
      "2024-03-26  0.960796 -1.177043 -0.384593  0.306386 -0.206326 -0.246683   \n",
      "2024-03-27  0.396878  1.991081  0.941800 -0.265792  1.179504  1.004419   \n",
      "2024-03-28 -0.749067  0.514492  0.585964  0.026648  1.496052  0.367481   \n",
      "\n",
      "                WDAY       XEL  \n",
      "Date                            \n",
      "2023-03-29  2.277783  1.269711  \n",
      "2023-03-30  0.389915  0.430715  \n",
      "2023-03-31  1.611597  0.597545  \n",
      "2023-04-03 -0.634285  0.121519  \n",
      "2023-04-04 -0.537767  1.011197  \n",
      "...              ...       ...  \n",
      "2024-03-22  0.106149 -0.002851  \n",
      "2024-03-25 -0.427841  0.321648  \n",
      "2024-03-26  0.237594 -0.878121  \n",
      "2024-03-27 -0.793726  2.193537  \n",
      "2024-03-28 -0.251144  0.527595  \n",
      "\n",
      "[252 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q13\n",
    "\n",
    "# Define the start and end dates for the period\n",
    "start_date = '2023-03-29'\n",
    "end_date = '2024-03-28'\n",
    "\n",
    "# Filter the DataFrame for the specified period\n",
    "df_filtered_new = df_filtered_nasdaq_100[start_date:end_date]\n",
    "\n",
    "# Calculate the mean and standard deviation for each stock over the specified period\n",
    "mean = df_filtered_new.mean()\n",
    "std_dev = df_filtered_new.std()\n",
    "\n",
    "# Standardize the returns\n",
    "Z = (df_filtered_new - mean) / std_dev\n",
    "\n",
    "# Print the resulting matrix Z\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Quv2tOhXch4-"
   },
   "source": [
    "---\n",
    "<font color=green>Q14: (1 Mark) </font>\n",
    "<br><font color='green'>\n",
    "Download the `Z_matrix` matrix from the course's GitHub account.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NmSz-J_oxYQa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
      "Date                                                                     \n",
      "2023-03-29  0.658925  2.189521  0.106285  0.207821  1.503570  0.444433   \n",
      "2023-03-30  0.273051 -0.221306 -0.389329 -0.434766  0.787034  0.526996   \n",
      "2023-03-31  0.360570  1.136308  1.540732  1.439604  0.531735 -0.056439   \n",
      "2023-04-03 -0.713053 -2.259591  0.252735  0.407399 -0.591892 -0.600163   \n",
      "2023-04-04  0.560730 -1.137430  0.099652  0.013877  0.658632 -0.342217   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-03-22 -1.146803 -0.516681  1.151431  1.085070  0.074915  0.081847   \n",
      "2024-03-25  0.659349 -1.221008 -0.372489 -0.341074  0.109668 -0.292706   \n",
      "2024-03-26 -0.032708  0.234343  0.131651  0.109342 -0.556130 -0.244717   \n",
      "2024-03-27 -0.363721  1.052056 -0.024166 -0.010591  0.315892  0.224881   \n",
      "2024-03-28 -0.048375  0.411934 -0.078409  0.019962  0.022729  0.067776   \n",
      "\n",
      "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
      "Date                                                ...                       \n",
      "2023-03-29  1.020467  0.727398  1.860839  0.356414  ...  0.517876  0.615242   \n",
      "2023-03-30  0.277291  0.076716  1.630406  0.936497  ... -0.109884  0.439972   \n",
      "2023-03-31  0.475365  0.008639  0.935425  1.044069  ...  1.337544  0.117646   \n",
      "2023-04-03 -0.086824  0.759731 -0.328378 -0.555499  ... -0.377687  1.191774   \n",
      "2023-04-04  0.223111  0.872403 -0.399698 -0.127907  ...  1.434972 -0.347689   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "2024-03-22 -0.150701 -0.278013 -0.555224  0.126867  ...  0.046870 -0.246035   \n",
      "2024-03-25 -0.084892  1.184710 -0.959285 -0.282029  ... -2.575803  0.240999   \n",
      "2024-03-26 -0.377797  0.183364 -0.577464  0.317533  ...  0.150958 -0.070195   \n",
      "2024-03-27  2.192442  1.128111  1.411127 -0.309662  ...  0.034693  0.474269   \n",
      "2024-03-28  1.190631 -0.583106  1.407557 -0.141870  ...  0.577964  0.645940   \n",
      "\n",
      "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
      "Date                                                                     \n",
      "2023-03-29  0.814986  1.344650  1.105776  0.127319  0.497115  0.429736   \n",
      "2023-03-30  0.233493  1.187056  0.239468 -0.525980  0.691243  0.446100   \n",
      "2023-03-31  2.058867  0.640237  0.325258  0.538943 -0.008816  0.565842   \n",
      "2023-04-03 -2.030038 -0.684867 -0.214460  0.183344  1.205798 -0.547975   \n",
      "2023-04-04 -0.377655 -1.394525 -0.538698 -0.487155  0.553153  0.755053   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-03-22 -0.386620 -0.054031 -0.476030 -0.091837 -0.421327 -0.946797   \n",
      "2024-03-25  0.343241 -0.651297 -1.151639 -0.024341  0.166127  0.118796   \n",
      "2024-03-26  0.960796 -1.177043 -0.384593  0.306386 -0.206326 -0.246683   \n",
      "2024-03-27  0.396878  1.991081  0.941800 -0.265792  1.179504  1.004419   \n",
      "2024-03-28 -0.749067  0.514492  0.585964  0.026648  1.496052  0.367481   \n",
      "\n",
      "                WDAY       XEL  \n",
      "Date                            \n",
      "2023-03-29  2.277783  1.269711  \n",
      "2023-03-30  0.389915  0.430715  \n",
      "2023-03-31  1.611597  0.597545  \n",
      "2023-04-03 -0.634285  0.121519  \n",
      "2023-04-04 -0.537767  1.011197  \n",
      "...              ...       ...  \n",
      "2024-03-22  0.106149 -0.002851  \n",
      "2024-03-25 -0.427841  0.321648  \n",
      "2024-03-26  0.237594 -0.878121  \n",
      "2024-03-27 -0.793726  2.193537  \n",
      "2024-03-28 -0.251144  0.527595  \n",
      "\n",
      "[252 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q14\n",
    "\n",
    "Z_matrix =pd.read_csv(\"https://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/Z_matrix.csv\",index_col=0, parse_dates=True)\n",
    "\n",
    "#Display the Z_matrix from the Github account\n",
    "print(Z_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d-2MrrzatMc"
   },
   "source": [
    "---\n",
    "<font color=green>Q15: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "For the specified period from March 29, 2023 ('2023-03-29'), to March 28, 2024 ('2024-03-28'), compute the correlation matrix\n",
    "$C$ using the matrix `Z_matrix`.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2rRt-HL1xZqA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ADBE       ADP     GOOGL      GOOG      AMZN       AMD       AEP  \\\n",
      "ADBE   1.000000  0.218513  0.397890  0.400601  0.463488  0.444032 -0.035967   \n",
      "ADP    0.218513  1.000000  0.294213  0.298841  0.168206  0.045884  0.228457   \n",
      "GOOGL  0.397890  0.294213  1.000000  0.997415  0.521199  0.371105 -0.006803   \n",
      "GOOG   0.400601  0.298841  0.997415  1.000000  0.525626  0.371568 -0.004037   \n",
      "AMZN   0.463488  0.168206  0.521199  0.525626  1.000000  0.463049 -0.010849   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "VRTX   0.164760  0.176771  0.142447  0.146190  0.104962  0.039540  0.239861   \n",
      "WBA    0.033955  0.142369  0.052710  0.060822  0.017926  0.002629  0.309717   \n",
      "WBD    0.099841  0.243986  0.042072  0.045516  0.162937  0.092733  0.325463   \n",
      "WDAY   0.418110  0.320836  0.289137  0.293575  0.403757  0.334587  0.017659   \n",
      "XEL    0.019105  0.164682  0.025701  0.026392 -0.058870 -0.214673  0.617318   \n",
      "\n",
      "           AMGN       ADI      ANSS  ...      TTWO      TMUS      TSLA  \\\n",
      "ADBE   0.198781  0.321991  0.387483  ...  0.257931  0.102167  0.268863   \n",
      "ADP    0.214813  0.279607  0.238355  ...  0.290311  0.113985  0.178128   \n",
      "GOOGL  0.118938  0.222252  0.292286  ...  0.238219  0.086673  0.267941   \n",
      "GOOG   0.118296  0.223710  0.294542  ...  0.242111  0.091456  0.268114   \n",
      "AMZN   0.123745  0.290872  0.342042  ...  0.222346  0.120301  0.303368   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "VRTX   0.281759  0.110189  0.142121  ...  0.180810  0.139184  0.144443   \n",
      "WBA    0.214701  0.208907  0.096813  ...  0.115189  0.063538  0.168495   \n",
      "WBD    0.220342  0.310681  0.095919  ...  0.129023  0.084315  0.282265   \n",
      "WDAY   0.068097  0.315426  0.382360  ...  0.293949  0.142648  0.277744   \n",
      "XEL    0.283399  0.029036  0.072063  ...  0.074596  0.249106  0.023168   \n",
      "\n",
      "            TXN      VRSK      VRTX       WBA       WBD      WDAY       XEL  \n",
      "ADBE   0.326597  0.171580  0.164760  0.033955  0.099841  0.418110  0.019105  \n",
      "ADP    0.297954  0.325258  0.176771  0.142369  0.243986  0.320836  0.164682  \n",
      "GOOGL  0.192188  0.178622  0.142447  0.052710  0.042072  0.289137  0.025701  \n",
      "GOOG   0.198044  0.180110  0.146190  0.060822  0.045516  0.293575  0.026392  \n",
      "AMZN   0.299500  0.144325  0.104962  0.017926  0.162937  0.403757 -0.058870  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "VRTX   0.198258  0.251863  1.000000  0.159124  0.062726  0.101851  0.184369  \n",
      "WBA    0.199627  0.038371  0.159124  1.000000  0.361533  0.010855  0.194839  \n",
      "WBD    0.355450  0.002990  0.062726  0.361533  1.000000  0.160860  0.183837  \n",
      "WDAY   0.346161  0.195588  0.101851  0.010855  0.160860  1.000000 -0.019310  \n",
      "XEL    0.062496  0.151549  0.184369  0.194839  0.183837 -0.019310  1.000000  \n",
      "\n",
      "[89 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q15\n",
    "\n",
    "#Calculate Transpose of matrix Z\n",
    "Z_transpose = Z_matrix.T\n",
    "\n",
    "#Calculate Correlation Matrix\n",
    "correlation_matrix = (1/(length_history-1)) * Z_transpose.dot(Z_matrix)\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FnUvUEkjAbG"
   },
   "source": [
    "---\n",
    "<font color=green>Q16: (2 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Refind the correlation matrix from the from March 29, 2023 ('2023-03-29'), to March 28, 2024 ('2024-03-28') using pandas correlation matrix method.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "T_g-VjITxasb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ADBE       ADP     GOOGL      GOOG      AMZN       AMD       AEP  \\\n",
      "ADBE   1.000000  0.218513  0.397890  0.400601  0.463488  0.444032 -0.035967   \n",
      "ADP    0.218513  1.000000  0.294213  0.298841  0.168206  0.045884  0.228457   \n",
      "GOOGL  0.397890  0.294213  1.000000  0.997415  0.521199  0.371105 -0.006803   \n",
      "GOOG   0.400601  0.298841  0.997415  1.000000  0.525626  0.371568 -0.004037   \n",
      "AMZN   0.463488  0.168206  0.521199  0.525626  1.000000  0.463049 -0.010849   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "VRTX   0.164760  0.176771  0.142447  0.146190  0.104962  0.039540  0.239861   \n",
      "WBA    0.033955  0.142369  0.052710  0.060822  0.017926  0.002629  0.309717   \n",
      "WBD    0.099841  0.243986  0.042072  0.045516  0.162937  0.092733  0.325463   \n",
      "WDAY   0.418110  0.320836  0.289137  0.293575  0.403757  0.334587  0.017659   \n",
      "XEL    0.019105  0.164682  0.025701  0.026392 -0.058870 -0.214673  0.617318   \n",
      "\n",
      "           AMGN       ADI      ANSS  ...      TTWO      TMUS      TSLA  \\\n",
      "ADBE   0.198781  0.321991  0.387483  ...  0.257931  0.102167  0.268863   \n",
      "ADP    0.214813  0.279607  0.238355  ...  0.290311  0.113985  0.178128   \n",
      "GOOGL  0.118938  0.222252  0.292286  ...  0.238219  0.086673  0.267941   \n",
      "GOOG   0.118296  0.223710  0.294542  ...  0.242111  0.091456  0.268114   \n",
      "AMZN   0.123745  0.290872  0.342042  ...  0.222346  0.120301  0.303368   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "VRTX   0.281759  0.110189  0.142121  ...  0.180810  0.139184  0.144443   \n",
      "WBA    0.214701  0.208907  0.096813  ...  0.115189  0.063538  0.168495   \n",
      "WBD    0.220342  0.310681  0.095919  ...  0.129023  0.084315  0.282265   \n",
      "WDAY   0.068097  0.315426  0.382360  ...  0.293949  0.142648  0.277744   \n",
      "XEL    0.283399  0.029036  0.072063  ...  0.074596  0.249106  0.023168   \n",
      "\n",
      "            TXN      VRSK      VRTX       WBA       WBD      WDAY       XEL  \n",
      "ADBE   0.326597  0.171580  0.164760  0.033955  0.099841  0.418110  0.019105  \n",
      "ADP    0.297954  0.325258  0.176771  0.142369  0.243986  0.320836  0.164682  \n",
      "GOOGL  0.192188  0.178622  0.142447  0.052710  0.042072  0.289137  0.025701  \n",
      "GOOG   0.198044  0.180110  0.146190  0.060822  0.045516  0.293575  0.026392  \n",
      "AMZN   0.299500  0.144325  0.104962  0.017926  0.162937  0.403757 -0.058870  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "VRTX   0.198258  0.251863  1.000000  0.159124  0.062726  0.101851  0.184369  \n",
      "WBA    0.199627  0.038371  0.159124  1.000000  0.361533  0.010855  0.194839  \n",
      "WBD    0.355450  0.002990  0.062726  0.361533  1.000000  0.160860  0.183837  \n",
      "WDAY   0.346161  0.195588  0.101851  0.010855  0.160860  1.000000 -0.019310  \n",
      "XEL    0.062496  0.151549  0.184369  0.194839  0.183837 -0.019310  1.000000  \n",
      "\n",
      "[89 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q16\n",
    "\n",
    "#Use Pandas Method\n",
    "correlation_matrix_pandas = Z_matrix.corr()\n",
    "\n",
    "# Display the correlation matrix \n",
    "print(correlation_matrix_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvsiMvgfaxzW"
   },
   "source": [
    "---\n",
    "<font color=green>Q17: (7 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Conduct Singular Value Decomposition on the correlation matrix $C$. Follow these steps:\n",
    "\n",
    "\n",
    "1.   **Perform SVD**: Decompose the matrix $C$ into its singular values and vectors.\n",
    "2.   **Rank Eigenvalues**: Sort the resulting singular values (often squared to compare to eigenvalues) in descending order.\n",
    "3. **Select Components**: Extract the first 20 components based on the largest singular values.\n",
    "4. **Variance Explained**: Print the variance explained by the first 20 Components and dimensions of differents matrix that you created.\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pohRSmSExbkq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimensions of the matrices:\n",
      "U matrix: (89, 89)\n",
      "S (eigenvalues): (89,)\n",
      "Vt matrix: (89, 89)\n",
      "\n",
      "Top 20 components: [489.80714433  53.88241254  15.34254661   7.62427837   6.63908465\n",
      "   4.19347111   3.45696857   3.10609749   3.03317018   2.60015846\n",
      "   2.30307941   2.09724098   1.96779575   1.75439203   1.56970662\n",
      "   1.42293776   1.37379448   1.31867962   1.1929447    1.15649058]\n",
      "\n",
      "Explained Variance percentage:  [0.78617886 0.0864855  0.02462599 0.01223756 0.01065625 0.00673085\n",
      " 0.00554871 0.00498553 0.00486848 0.00417346 0.00369662 0.00336624\n",
      " 0.00315847 0.00281594 0.0025195  0.00228393 0.00220505 0.00211658\n",
      " 0.00191477 0.00185626]\n",
      "\n",
      "Variance explained by the first 20 components: 97.24245358638191 %\n"
     ]
    }
   ],
   "source": [
    "#Q17\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Perform SVD\n",
    "SVD = np.linalg.svd(correlation_matrix_pandas)\n",
    "\n",
    "# Calculate and Rank Eigenvalues\n",
    "eigenvalues = SVD[1]**2\n",
    "sorted_eigenvalues = np.argsort(eigenvalues)[::-1]\n",
    "\n",
    "# Select the first 20 eigenvalues\n",
    "top_20_indices = sorted_eigenvalues[:20]\n",
    "top_20_eigenvalues = eigenvalues[top_20_indices]\n",
    "\n",
    "# Find variance of model explained by the first 20 eigenvalues\n",
    "explained_variance = np.sum(top_20_eigenvalues) / np.sum(eigenvalues)\n",
    "explained_variance_pct = top_20_eigenvalues / np.sum(eigenvalues)\n",
    "\n",
    "# Print dimensions of matrices\n",
    "print(\"\\nDimensions of the matrices:\")\n",
    "print(\"U matrix:\", SVD[0].shape)\n",
    "print(\"S (eigenvalues):\", SVD[1].shape)\n",
    "print(\"Vt matrix:\", SVD[2].shape)\n",
    "\n",
    "# Print the top 20 eigenvalues\n",
    "print(\"\\nTop 20 components:\",top_20_eigenvalues)\n",
    "\n",
    "print ('\\nExplained Variance percentage: ', explained_variance_pct)\n",
    "\n",
    "\n",
    "# Print the variance explained by the first 20 components\n",
    "print(\"\\nVariance explained by the first 20 components:\",explained_variance*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3sZ7K-Tb6S_"
   },
   "source": [
    "---\n",
    "<font color=green>Q18: (6 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Extract the 20 hidden factors in a matrix F. Check that shape of F is $(252,20)$\n",
    "</font>\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of F Matrix: (252, 20)\n",
      "[[-10.42428276  -1.13807494  -1.48029262 ...   1.23955006   2.27676229\n",
      "   -0.13751588]\n",
      " [ -4.05837617   0.78059648  -0.86611386 ...  -0.36262048  -0.97011478\n",
      "   -0.91996453]\n",
      " [ -7.95739081  -2.27970763   1.48389609 ...   0.05732551  -0.36311986\n",
      "   -0.11338118]\n",
      " ...\n",
      " [  0.88061846   0.13735036   1.12187707 ...  -0.17879579   0.09227425\n",
      "    0.16559638]\n",
      " [ -5.48903391  -4.97318897  -3.13593018 ...  -1.29272596  -0.13181172\n",
      "    0.74442252]\n",
      " [ -0.52987874  -1.28782948  -0.79107298 ...  -0.58448879   0.61863981\n",
      "    0.69301072]]\n"
     ]
    }
   ],
   "source": [
    "#Q18\n",
    "\n",
    "# We extract the top 20 components from the V transpose(right singular) matrix of SVD:\n",
    "top_20_eigenvectors = SVD[2][top_20_indices, :].T\n",
    "\n",
    "#Get a new dataframe with the standard deviations of returns for each stock:\n",
    "sigma_returns = df_filtered_new.std()\n",
    "\n",
    "#Create an initial F matrix\n",
    "F =np.zeros((length_history,20))\n",
    "\n",
    "#Use the provided formula to calculate the F matrix top 20 hidden factors\n",
    "for j in range(20):\n",
    "    F[:, j] = np.dot(df_filtered_new.values, top_20_eigenvectors[:, j] / sigma_returns.values)\n",
    "\n",
    "# Double check the shape of the F Matrix\n",
    "print(\"Shape of F Matrix:\", F.shape)  # Must be (252, 20)\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz-ncrUmCAVW"
   },
   "source": [
    "---\n",
    "<font color=green>Q19: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Perform the Regression Analysis of 'AAPL' for the date '2024-03-28', using a historical data length of 252 days using previous $F$ Matrix. Compare the R-squared from the ones obtained at Q11.\n",
    "</font>\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vcPG6YWoxdgn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   AAPL   R-squared:                       0.615\n",
      "Model:                            OLS   Adj. R-squared:                  0.582\n",
      "Method:                 Least Squares   F-statistic:                     18.48\n",
      "Date:                Thu, 06 Jun 2024   Prob (F-statistic):           2.29e-37\n",
      "Time:                        17:50:55   Log-Likelihood:                 873.13\n",
      "No. Observations:                 252   AIC:                            -1704.\n",
      "Df Residuals:                     231   BIC:                            -1630.\n",
      "Df Model:                          20                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0005      0.001     -0.874      0.383      -0.001       0.001\n",
      "0             -0.0015      0.000    -14.408      0.000      -0.002      -0.001\n",
      "1              0.0002      0.000      0.981      0.327      -0.000       0.001\n",
      "2              0.0012      0.000      4.741      0.000       0.001       0.002\n",
      "3             -0.0003      0.000     -0.954      0.341      -0.001       0.000\n",
      "4             -0.0014      0.000     -4.351      0.000      -0.002      -0.001\n",
      "5             -0.0006      0.000     -1.632      0.104      -0.001       0.000\n",
      "6             -0.0010      0.000     -2.862      0.005      -0.002      -0.000\n",
      "7             -0.0024      0.000     -6.338      0.000      -0.003      -0.002\n",
      "8           2.647e-05      0.000      0.070      0.944      -0.001       0.001\n",
      "9             -0.0017      0.000     -4.295      0.000      -0.002      -0.001\n",
      "10             0.0015      0.000      3.786      0.000       0.001       0.002\n",
      "11            -0.0012      0.000     -2.866      0.005      -0.002      -0.000\n",
      "12         -3.566e-05      0.000     -0.085      0.933      -0.001       0.001\n",
      "13            -0.0004      0.000     -0.860      0.390      -0.001       0.000\n",
      "14             0.0020      0.000      4.411      0.000       0.001       0.003\n",
      "15             0.0007      0.000      1.490      0.138      -0.000       0.002\n",
      "16             0.0006      0.000      1.202      0.231      -0.000       0.001\n",
      "17             0.0004      0.000      0.915      0.361      -0.000       0.001\n",
      "18            -0.0005      0.000     -1.138      0.256      -0.001       0.000\n",
      "19            -0.0004      0.000     -0.853      0.395      -0.001       0.001\n",
      "==============================================================================\n",
      "Omnibus:                       14.605   Durbin-Watson:                   1.977\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               25.419\n",
      "Skew:                          -0.323   Prob(JB):                     3.02e-06\n",
      "Kurtosis:                       4.415   Cond. No.                         5.07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "R-squared from PCA model: 61.540204797502305 %\n",
      "R-squared from Fama-French model: 38.52518241205336 %\n"
     ]
    }
   ],
   "source": [
    "#Q19\n",
    "\n",
    "# Utilize relevant data for AAPL\n",
    "aapl_returns = df_filtered_new['AAPL']\n",
    "F_regression = pd.DataFrame(F, index=aapl_returns.index)\n",
    "\n",
    "# Perform the regression\n",
    "X = F_regression\n",
    "X = sm.add_constant(X)\n",
    "y = aapl_returns\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print the summary of the regression model\n",
    "print(model.summary())\n",
    "\n",
    "# Compare R-squared from the previous Fama-French regression in Q12\n",
    "print(f\"R-squared from PCA model:\", model.rsquared*100,\"%\")\n",
    "print(f\"R-squared from Fama-French model:\", ff_results_apple[3]*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDU8xmpi_ueR"
   },
   "source": [
    "# Ornstein Uhlenbeck\n",
    "\n",
    "The Ornstein-Uhlenbeck process is defined by the following stochastic differential equation (SDE):\n",
    "\n",
    "$$ dX_t = \\theta (\\mu - X_t) dt + \\sigma dW_t $$\n",
    "\n",
    "where:\n",
    "\n",
    "- **$ X_t $**: The value of the process at time $ t $.\n",
    "- **$ \\mu $**: The long-term mean (equilibrium level) to which the process reverts.\n",
    "- **$ \\theta $**: The speed of reversion or the rate at which the process returns to the mean.\n",
    "- **$ \\sigma $**: The volatility (standard deviation), representing the magnitude of random fluctuations.\n",
    "- **$ W_t $**: A Wiener process or Brownian motion that adds stochastic (random) noise.\n",
    "\n",
    "This equation describes a process where the variable $ X_t $ moves towards the mean $ \\mu $ at a rate determined by $ \\theta $, with random noise added by $ \\sigma dW_t $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HMYOiWsP53c"
   },
   "source": [
    "---\n",
    "<font color=green>Q20: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "In the context of mean reversion, which quantity should be modeled using an Ornstein-Uhlenbeck process?\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiO01w7fO_bR"
   },
   "source": [
    "The quantity that should be modeled using and Ornstein-Uhlenbeck process is the residuals of the time series for several reasons. First, the solution to the model is in a closed form, making it tractable without needing to apply numerical approximations. Also, since residual are assumed to be stochastic, the model accurately replicates that by assuming that the residuals follow a standard normal distribution under this process. Furthermore, the first and second moments (mean and variance) are defined, and the process is mean reverting, which sets a limit for the drift. All these assumptions and properties make the model favourable for describing the behaviour of the residuals in time series analysis while maintaining simplicity and trackability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t31Q2iWgQgsO"
   },
   "source": [
    "---\n",
    "<font color=green>Q21: (5 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Explain how the parameters $ \\theta $ and $ \\sigma $ can be determined using the following equations. Also, detail the underlying assumptions:\n",
    "$$ E[X] = \\mu $$\n",
    "$$ \\text{Var}[X] = \\frac{\\sigma^2}{2\\theta} $$\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_KjMbQplj4U"
   },
   "source": [
    "### Determining the parameters $\\theta$ and $\\sigma$ \n",
    "\n",
    "The parameters $\\theta$ and $\\sigma$ can be determined using maximum likelihood estimation. Knowing that the solution to the stochastic differential equation (SDE) is in continuous time, we need to transition into a discrete time framework. \n",
    "\n",
    "After modelling the stock returns, we extract the daily residuals and calculate their first difference as a proxy for $dX_t$. Then we either assume that the residuals follow a certain distribution such as the standard normal distribution, or we perform tests to extract the properties on their distribution. \n",
    "\n",
    "We then write the log likelihood of the parameters and maximize that function using argmax:\n",
    "\n",
    "$$\\theta^*, sigma^* = \\text{argmax}_{\\theta, \\sigma} \\ell(\\theta, \\sigma; \\{X_{t_i}\\})$$\n",
    "\n",
    "where $\\ell$ is the log-likelihood function estimated based on the density function of the residuals. This maximization can be achieved using numerical methods or using gradient descent. Finally, the obtained $\\theta^*$ and $\\sigma^*$ are used as the parameter in the Ornstein-Uhlenbeck process to model the residual time series. \n",
    "\n",
    "\n",
    "\n",
    "### Analysis of the Model\n",
    "\n",
    "To understand the properties of the model, we must first solve the stochatic differential equation, then we must compute the unconditional expectation and variance of $X_t$.\n",
    "\n",
    "The SDE for the Ornstein-Uhlenbeck process is given by:\n",
    "$$dX_t = \\theta (\\mu - X_t) dt + \\sigma dW_t$$\n",
    "\n",
    "$$dX_t = \\theta \\mu dt - \\theta X_t dt + \\sigma dW_t$$\n",
    "\n",
    "\n",
    "Multiply the equation by the Intergrating factor $e^{\\theta t}$:\n",
    "\n",
    "$$e^{\\theta t} dX_t = e^{\\theta t} \\theta \\mu dt - e^{\\theta t} \\theta X_t dt + e^{\\theta t} \\sigma dW_t$$\n",
    "\n",
    "Note:\n",
    "$$\\frac{d}{dt}(X_t e^{\\theta t}) = e^{\\theta t} \\frac{dX_t}{dt} + X_t \\frac{d}{dt}(e^{\\theta t})$$\n",
    "\n",
    "Thus: \n",
    "$$\\frac{d}{dt}(X_t e^{\\theta t}) = \\theta \\mu e^{\\theta t} + \\sigma e^{\\theta t} \\frac{dW_t}{dt}$$\n",
    "\n",
    "\n",
    "$$ dX_t e^{\\theta t} = \\theta \\mu e^{\\theta t}dt + \\sigma e^{\\theta t}dW_t$$\n",
    "\n",
    "\n",
    "Integrate both sides from $t$ to $T$:\n",
    "$$\\int_t^T d(X_s e^{\\theta s}) = \\int_t^T \\theta \\mu e^{\\theta s} ds +\\int_t^T \\sigma e^{\\theta s} dW_s$$\n",
    "$$X_T e^{\\theta T} - X_t e^{\\theta t} = \\mu e^{\\theta T} - \\mu e^{\\theta t} + \\int_t^T \\sigma e^{\\theta s} dW_s$$\n",
    "\n",
    "Isolating $X_t$:\n",
    "$$X_T = X_t e^{-\\theta (T- t)} + \\mu (1 - e^{-\\theta (T-t)}) + \\sigma e^{-\\theta T} \\int_t^T e^{\\theta s} dW_s$$\n",
    "\n",
    "Let $T=t$ and $t=0$:\n",
    "\n",
    "$$X_t = X_0 e^{-\\theta t} + \\mu (1 - e^{-\\theta t}) + \\sigma e^{-\\theta t} \\int_0^t e^{\\theta s} dW_s$$\n",
    "\n",
    "\n",
    "### Expectation of $X_t$:\n",
    "\n",
    "To obtain the expectation of $X_t$, we take the expectation of our solution on both sides:\n",
    "\n",
    "$$E[X_t] = E[X_0] e^{-\\theta t} + \\mu (1 - e^{-\\theta t}) + 0$$\n",
    "\n",
    "$$E[X_t] = \\mu e^{-\\theta t} + \\mu - \\mu e^{-\\theta t}$$\n",
    "\n",
    "$$E[X_t] = \\mu$$\n",
    "\n",
    "### Explanation: \n",
    "\n",
    "$\\mu$ is the mean of the model, which is also the unconditional expectation of $X_t$. Looking back at the initial equation describing the Ornstein-Uhlenbeck process, we can realize that this model implies that whenever $X_t$ diverts from the mean $\\mu$, it will converge back to $\\mu$ at a speed of $\\theta$. For that reason, the model is designed such that the unconditional expectation is the mean, highlighting this property of mean reversion. For the case of residual time series, the error follows a brownian motion with a mean-reverting component which indicates that the residual time series converges over time to an expected value equal to its mean, which could be 0 to satisfy the unbias assumption since we are dealing with time series modeling of the residuals and not of stock returns themselves.\n",
    "\n",
    "\n",
    "\n",
    "### Variance of $X_t$\n",
    "\n",
    "We now calculate the unconditional variance of $X_t$. Recall the following: \n",
    "\n",
    "$$X_t = X_0 e^{-\\theta t} + \\mu (1 - e^{-\\theta t}) + \\sigma e^{-\\theta t} \\int_0^t e^{\\theta s} dW_s$$\n",
    "\n",
    "We know that $V(X_t) = E[(X_t)^2]-(E[X_t])^2$\n",
    "\n",
    "Hence, we start by calculating $(X_t)^2$:\n",
    "\n",
    "$$(X_t)^2 = [X_0 e^{-\\theta t} + \\mu (1 - e^{-\\theta t}) + \\sigma e^{-\\theta t} \\int_0^t e^{\\theta s} dW_s]\\times  [X_0 e^{-\\theta t} + \\mu (1 - e^{-\\theta t}) + \\sigma e^{-\\theta t} \\int_0^t e^{\\theta s} dW_s]$$\n",
    "\n",
    "\n",
    "$$(X_t)^2 = X^2_0 e^{-2\\theta t}+ 2X_0\\mu e^{-\\theta t} (1 - e^{-\\theta t})+2X_0\\sigma e^{-2\\theta t} \\int_0^t e^{\\theta s} dW_s + u^2 (1-e^{-\\theta t})^2 + 2\\mu\\sigma(1-e^{-\\theta t})e^{-\\theta t}\\int_0^t e^{\\theta s} dW_s + \\sigma^2 e^{-2\\theta t} (\\int_0^t e^{\\theta s} dW_s)^2$$\n",
    "\n",
    "Note that according to Ito's multiplication rule, $(dW_s)^2 = ds$. Thus:\n",
    "\n",
    "$$(\\int_0^t e^{\\theta s} dW_s)^2 = \\int_0^t e^{2\\theta s} ds =  \\frac{1}{2\\theta} (e^{2\\theta t}-1)$$\n",
    "\n",
    "Plug into the equation of $(X_t)^2$:\n",
    "\n",
    "$$(X_t)^2 = X^2_0 e^{-2\\theta t}+ 2X_0\\mu e^{-\\theta t} (1 - e^{-\\theta t})+2X_0\\sigma e^{-2\\theta t} \\int_0^t e^{\\theta s} dW_s + u^2 (1-e^{-\\theta t})^2 + 2\\mu\\sigma(1-e^{-\\theta t})e^{-\\theta t}\\int_0^t e^{\\theta s} dW_s + \\frac{\\sigma^2}{2\\theta} e^{-2\\theta t}   (e^{2\\theta t}-1)$$\n",
    "\n",
    "Note: $\\frac{\\sigma^2}{2\\theta} e^{-2\\theta t}(e^{2\\theta t}-1) = \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t})$\n",
    "\n",
    "Thus: \n",
    "\n",
    "$$(X_t)^2 = X^2_0 e^{-2\\theta t}+ 2X_0\\mu e^{-\\theta t} (1 - e^{-\\theta t})+ 2X_0\\sigma e^{-2\\theta t} \\int_0^t e^{\\theta s} dW_s + u^2 (1-e^{-\\theta t})^2 + 2\\mu\\sigma(1-e^{-\\theta t})e^{-\\theta t}\\int_0^t e^{\\theta s} dW_s + \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t})$$\n",
    "\n",
    "We now take the expectation of $(X_t)^2$. Note that the expectation of a brownian motion is 0. Also, the expectation of the mean $\\mu$ is itself. Thus: \n",
    "\n",
    "$$E[X_t]^2 = E[X^2_0] e^{-2\\theta t} + E[2X_0]\\mu e^{-\\theta t}(1 - e^{-\\theta t}) + u^2 (1-e^{-\\theta t})^2 + \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t})$$ \n",
    "\n",
    "Moreover, we know that $E[X_t] = \\mu$. We also set that   $E[X_0] = \\mu$. furthermore, since $X_0$ is known, we conclude that  $E[X_0]^2 = \\mu^2$. Hence: \n",
    "\n",
    "$$E[X_t]^2 = u^2 e^{-2\\theta t} + 2\\mu^2  e^{-\\theta t}(1 - e^{-\\theta t}) + u^2 (1-e^{-\\theta t})^2 + \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t})$$ \n",
    "\n",
    "$$E[X_t]^2 = u^2 e^{-2\\theta t} + 2\\mu^2  e^{-\\theta t} - 2\\mu^2  e^{-2\\theta t} + u^2 -2u^2e^{-\\theta t}+u^2e^{-2\\theta t} + \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t})$$\n",
    "\n",
    "All $\\mu^2$ terms cancel out except for $\\mu^2$:\n",
    "\n",
    "$$E[X_t]^2 = \\mu^2 + \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t})$$\n",
    "\n",
    "We now compute the variance: \n",
    "\n",
    "$$V(X_t) = E[(X_t)^2]-(E[X_t])^2 = \\mu^2 + \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t}) - \\mu^2$$\n",
    "\n",
    "$$V(X_t) = \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t})$$\n",
    "\n",
    "Finally, to compute the unconditional variance, we evaluate the limit of $V(X_t)$ as $t$ tends to $\\infty$: \n",
    "\n",
    "$$V(X) = \\lim_{t \\to \\infty} V_t = \\lim_{t \\to \\infty} \\frac{\\sigma^2}{2\\theta} \\left[ 1 - e^{-2\\theta t} \\right]$$\n",
    "\n",
    "$$V(X) = \\frac{\\sigma^2}{2\\theta}$$\n",
    "\n",
    "### Explanation\n",
    "\n",
    "The variance of $X_t$ in the long term is dependent on two factors: the volatility $\\sigma$ and the speed of mean reversion $\\theta$. This means that over time we expect $X_t$ to disperse from the mean by $\\frac{\\sigma^2}{2\\theta}$ on average. as the volatility increases or the speed of mean reversion decreases (or both), the variance increases, meaning that the residuals of the time series data will be more dispered and will take a higher range of values away from the mean. This is logical as for high values of $\\theta$, the mean reversion component will prevent the residuals from being much larger or much lower from the mean. Also, a greater diffusion coefficient causes the residuals to overcome the mean reversion factor and increase its range of possible values in a stochastic manner before being pulled back towards the mean value. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JvTPsTTSMp4"
   },
   "source": [
    "---\n",
    "<font color=green>Q22: (2 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Create a function named `extract_s_scores` which computes 's scores' for the last element in a list of floating-point numbers. This function calculates the scores using the following formula $ \\text{s scores} = \\frac{X_T - \\mu}{\\sigma} $ where `list_xi` represents a list containing a sequence of floating-point numbers $(X_0, \\cdots, X_T)$.\n",
    "\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "unNHgTj8xkv5"
   },
   "outputs": [],
   "source": [
    "#Q22\n",
    "\n",
    "def extract_s_scores(list_xi):\n",
    "    Ïƒ = np.std(list_xi)\n",
    "    Î¼ = np.mean(list_xi)\n",
    "    XT = list_xi[len(list_xi)-1]\n",
    "    S_score= (XT-Î¼)/Ïƒ\n",
    "    return S_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAB_ANF2gCiY"
   },
   "source": [
    "# Autoencoder Analysis\n",
    "\n",
    "Autoencoders are neural networks used for unsupervised learning, particularly for dimensionality reduction and feature extraction. Training an autoencoder on the $Z_i$ matrix aims to identify hidden factors capturing the intrinsic structures in financial data.\n",
    "\n",
    "### Architecture\n",
    "- **Encoder**: Compresses input data into a smaller latent space representation.\n",
    "  - *Input Layer*: Matches the number of features in the $Z_i$ matrix.\n",
    "  - *Hidden Layers*: Compress data through progressively smaller layers.\n",
    "  - *Latent Space*: Encodes the data into hidden factors.\n",
    "- **Decoder**: Reconstructs input data from the latent space.\n",
    "  - *Hidden Layers*: Gradually expand to the original dimension.\n",
    "  - *Output Layer*: Matches the input layer to recreate the original matrix.\n",
    "\n",
    "### Training\n",
    "The autoencoder is trained by minimizing reconstruction loss, usually mean squared error (MSE), between the input $Z_i$ matrix and the decoder's output.\n",
    "\n",
    "### Hidden Factors Extraction\n",
    "After training, the encoder's latent space provides the most important underlying patterns in the stock returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJzNtknXgdqF"
   },
   "source": [
    "---\n",
    "<font color=green>Q23: (2 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Modify the standardized returns matrix `Z_matrix` to reduce the influence of extreme outliers on model trainingby ensuring that all values in the matrix `Z_matrix` do not exceed 3 standard deviations from the mean. Specifically, cap these values at the interval $-3, 3]$. Store the adjusted values in a new matrix, `Z_hat`.\n",
    "</font>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Ln7vWV0TxmFk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
      "Date                                                                     \n",
      "2023-03-29  0.658925  2.189521  0.106285  0.207821  1.503570  0.444433   \n",
      "2023-03-30  0.273051 -0.221306 -0.389329 -0.434766  0.787034  0.526996   \n",
      "2023-03-31  0.360570  1.136308  1.540732  1.439604  0.531735 -0.056439   \n",
      "2023-04-03 -0.713053 -2.259591  0.252735  0.407399 -0.591892 -0.600163   \n",
      "2023-04-04  0.560730 -1.137430  0.099652  0.013877  0.658632 -0.342217   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-03-22 -1.146803 -0.516681  1.151431  1.085070  0.074915  0.081847   \n",
      "2024-03-25  0.659349 -1.221008 -0.372489 -0.341074  0.109668 -0.292706   \n",
      "2024-03-26 -0.032708  0.234343  0.131651  0.109342 -0.556130 -0.244717   \n",
      "2024-03-27 -0.363721  1.052056 -0.024166 -0.010591  0.315892  0.224881   \n",
      "2024-03-28 -0.048375  0.411934 -0.078409  0.019962  0.022729  0.067776   \n",
      "\n",
      "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
      "Date                                                ...                       \n",
      "2023-03-29  1.020467  0.727398  1.860839  0.356414  ...  0.517876  0.615242   \n",
      "2023-03-30  0.277291  0.076716  1.630406  0.936497  ... -0.109884  0.439972   \n",
      "2023-03-31  0.475365  0.008639  0.935425  1.044069  ...  1.337544  0.117646   \n",
      "2023-04-03 -0.086824  0.759731 -0.328378 -0.555499  ... -0.377687  1.191774   \n",
      "2023-04-04  0.223111  0.872403 -0.399698 -0.127907  ...  1.434972 -0.347689   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "2024-03-22 -0.150701 -0.278013 -0.555224  0.126867  ...  0.046870 -0.246035   \n",
      "2024-03-25 -0.084892  1.184710 -0.959285 -0.282029  ... -2.575803  0.240999   \n",
      "2024-03-26 -0.377797  0.183364 -0.577464  0.317533  ...  0.150958 -0.070195   \n",
      "2024-03-27  2.192442  1.128111  1.411127 -0.309662  ...  0.034693  0.474269   \n",
      "2024-03-28  1.190631 -0.583106  1.407557 -0.141870  ...  0.577964  0.645940   \n",
      "\n",
      "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
      "Date                                                                     \n",
      "2023-03-29  0.814986  1.344650  1.105776  0.127319  0.497115  0.429736   \n",
      "2023-03-30  0.233493  1.187056  0.239468 -0.525980  0.691243  0.446100   \n",
      "2023-03-31  2.058867  0.640237  0.325258  0.538943 -0.008816  0.565842   \n",
      "2023-04-03 -2.030038 -0.684867 -0.214460  0.183344  1.205798 -0.547975   \n",
      "2023-04-04 -0.377655 -1.394525 -0.538698 -0.487155  0.553153  0.755053   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-03-22 -0.386620 -0.054031 -0.476030 -0.091837 -0.421327 -0.946797   \n",
      "2024-03-25  0.343241 -0.651297 -1.151639 -0.024341  0.166127  0.118796   \n",
      "2024-03-26  0.960796 -1.177043 -0.384593  0.306386 -0.206326 -0.246683   \n",
      "2024-03-27  0.396878  1.991081  0.941800 -0.265792  1.179504  1.004419   \n",
      "2024-03-28 -0.749067  0.514492  0.585964  0.026648  1.496052  0.367481   \n",
      "\n",
      "                WDAY       XEL  \n",
      "Date                            \n",
      "2023-03-29  2.277783  1.269711  \n",
      "2023-03-30  0.389915  0.430715  \n",
      "2023-03-31  1.611597  0.597545  \n",
      "2023-04-03 -0.634285  0.121519  \n",
      "2023-04-04 -0.537767  1.011197  \n",
      "...              ...       ...  \n",
      "2024-03-22  0.106149 -0.002851  \n",
      "2024-03-25 -0.427841  0.321648  \n",
      "2024-03-26  0.237594 -0.878121  \n",
      "2024-03-27 -0.793726  2.193537  \n",
      "2024-03-28 -0.251144  0.527595  \n",
      "\n",
      "[252 rows x 89 columns]\n",
      "The minimum observed value of Z is:  -3.0\n",
      "The maximum observed value of Z is:  3.0\n"
     ]
    }
   ],
   "source": [
    "#Q23\n",
    "\n",
    "Z_hat=Z_matrix\n",
    "\n",
    "Z_hat[Z_hat > 3] = 3\n",
    "Z_hat[Z_hat < -3] = -3\n",
    "\n",
    "#Print the DataFrame as well as the minimum and maximum value in it to check if it is consistent with the question\n",
    "print(Z_hat)\n",
    "print(\"The minimum observed value of Z is: \", np.min(Z_hat))\n",
    "print(\"The maximum observed value of Z is: \",np.max(Z_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNJDLYZ9g_V4"
   },
   "source": [
    "---\n",
    "<font color=green>Q24: (1 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Fetch the `Z_hat` data from GitHub, and we'll proceed with it now.\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BraH_nivxngd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ADBE       ADP     GOOGL      GOOG      AMZN       AMD  \\\n",
      "Date                                                                     \n",
      "2023-03-29  0.658925  2.189521  0.106285  0.207821  1.503570  0.444433   \n",
      "2023-03-30  0.273051 -0.221306 -0.389329 -0.434766  0.787034  0.526996   \n",
      "2023-03-31  0.360570  1.136308  1.540732  1.439604  0.531735 -0.056439   \n",
      "2023-04-03 -0.713053 -2.259591  0.252735  0.407399 -0.591892 -0.600163   \n",
      "2023-04-04  0.560730 -1.137430  0.099652  0.013877  0.658632 -0.342217   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-03-22 -1.146803 -0.516681  1.151431  1.085070  0.074915  0.081847   \n",
      "2024-03-25  0.659349 -1.221008 -0.372489 -0.341074  0.109668 -0.292706   \n",
      "2024-03-26 -0.032708  0.234343  0.131651  0.109342 -0.556130 -0.244717   \n",
      "2024-03-27 -0.363721  1.052056 -0.024166 -0.010591  0.315892  0.224881   \n",
      "2024-03-28 -0.048375  0.411934 -0.078409  0.019962  0.022729  0.067776   \n",
      "\n",
      "                 AEP      AMGN       ADI      ANSS  ...      TTWO      TMUS  \\\n",
      "Date                                                ...                       \n",
      "2023-03-29  1.020467  0.727398  1.860839  0.356414  ...  0.517876  0.615242   \n",
      "2023-03-30  0.277291  0.076716  1.630406  0.936497  ... -0.109884  0.439972   \n",
      "2023-03-31  0.475365  0.008639  0.935425  1.044069  ...  1.337544  0.117646   \n",
      "2023-04-03 -0.086824  0.759731 -0.328378 -0.555499  ... -0.377687  1.191774   \n",
      "2023-04-04  0.223111  0.872403 -0.399698 -0.127907  ...  1.434972 -0.347689   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "2024-03-22 -0.150701 -0.278013 -0.555224  0.126867  ...  0.046870 -0.246035   \n",
      "2024-03-25 -0.084892  1.184710 -0.959285 -0.282029  ... -2.575803  0.240999   \n",
      "2024-03-26 -0.377797  0.183364 -0.577464  0.317533  ...  0.150958 -0.070195   \n",
      "2024-03-27  2.192442  1.128111  1.411127 -0.309662  ...  0.034693  0.474269   \n",
      "2024-03-28  1.190631 -0.583106  1.407557 -0.141870  ...  0.577964  0.645940   \n",
      "\n",
      "                TSLA       TXN      VRSK      VRTX       WBA       WBD  \\\n",
      "Date                                                                     \n",
      "2023-03-29  0.814986  1.344650  1.105776  0.127319  0.497115  0.429736   \n",
      "2023-03-30  0.233493  1.187056  0.239468 -0.525980  0.691243  0.446100   \n",
      "2023-03-31  2.058867  0.640237  0.325258  0.538943 -0.008816  0.565842   \n",
      "2023-04-03 -2.030038 -0.684867 -0.214460  0.183344  1.205798 -0.547975   \n",
      "2023-04-04 -0.377655 -1.394525 -0.538698 -0.487155  0.553153  0.755053   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2024-03-22 -0.386620 -0.054031 -0.476030 -0.091837 -0.421327 -0.946797   \n",
      "2024-03-25  0.343241 -0.651297 -1.151639 -0.024341  0.166127  0.118796   \n",
      "2024-03-26  0.960796 -1.177043 -0.384593  0.306386 -0.206326 -0.246683   \n",
      "2024-03-27  0.396878  1.991081  0.941800 -0.265792  1.179504  1.004419   \n",
      "2024-03-28 -0.749067  0.514492  0.585964  0.026648  1.496052  0.367481   \n",
      "\n",
      "                WDAY       XEL  \n",
      "Date                            \n",
      "2023-03-29  2.277783  1.269711  \n",
      "2023-03-30  0.389915  0.430715  \n",
      "2023-03-31  1.611597  0.597545  \n",
      "2023-04-03 -0.634285  0.121519  \n",
      "2023-04-04 -0.537767  1.011197  \n",
      "...              ...       ...  \n",
      "2024-03-22  0.106149 -0.002851  \n",
      "2024-03-25 -0.427841  0.321648  \n",
      "2024-03-26  0.237594 -0.878121  \n",
      "2024-03-27 -0.793726  2.193537  \n",
      "2024-03-28 -0.251144  0.527595  \n",
      "\n",
      "[252 rows x 89 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q24\n",
    "\n",
    "Z_hat=pd.read_csv(\"https://raw.githubusercontent.com/Jandsy/ml_finance_imperial/main/Coursework/Z_hat.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "print(Z_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3MRBtXIiWcN"
   },
   "source": [
    "---\n",
    "<font color=green>Q25: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Segment the standardized and capped returns matrix $\\hat{Z}$ into two subsets for model training and testing. Precisly Allocate 70% of the data in $\\hat{Z}$ to the training set $ \\hat{Z}_{train} $ and Allocate the remaining 30% to the testing set $\\hat{Z}_{test}$. Treat each stock within $\\hat{Z}$ as an individual sample, by flattening temporal dependencies.\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_hat_train dimensions (252, 62) and Z_hat_test dimensions (252, 27)\n"
     ]
    }
   ],
   "source": [
    "#Q25\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming Z_hat is your dataframe and the columns represent companies\n",
    "\n",
    "# Calculate the threshold for 70% of the companies\n",
    "threshold = int(0.7 * Z_hat.shape[1])\n",
    "\n",
    "# Split into training and test data based on the columns (companies)\n",
    "Z_hat_train = Z_hat.iloc[:, :threshold].reset_index(drop=False)\n",
    "Z_hat_test = Z_hat.iloc[:, threshold:].reset_index(drop=False)\n",
    "\n",
    "# Turn the 'Date' column to datetime and set it as the index\n",
    "Z_hat_train[\"Date\"] = pd.to_datetime(Z_hat_train[\"Date\"])\n",
    "Z_hat_train.set_index(\"Date\", inplace=True)\n",
    "\n",
    "Z_hat_test[\"Date\"] = pd.to_datetime(Z_hat_test[\"Date\"])\n",
    "Z_hat_test.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Print the training and test sets as well as the shapes to double check\n",
    "print('Z_hat_train dimensions', Z_hat_train.shape, 'and Z_hat_test dimensions' , Z_hat_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqWeD_efihbH"
   },
   "source": [
    "---\n",
    "<font color=green>Q26: (10 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Please create an autoencoder following the instructions provided in  **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**, Use the model 'Variant 2' in Table 1.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hyf_dfsoxp_X"
   },
   "outputs": [],
   "source": [
    "#Q26\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "#Determine the input shape from Z_hat by transposing\n",
    "\n",
    "input_shape = Z_hat_train.T.shape[1] #Number of features in the input data\n",
    "\n",
    "input_layer = layers.Input(shape=(input_shape,))\n",
    "    \n",
    "#Encoding layer (using tanh activation)\n",
    "encoded = layers.Dense(20, activation='tanh', use_bias=True)(input_layer) \n",
    "    \n",
    "#Decoding layer (using tanh activation)\n",
    "decoded = layers.Dense(input_shape, activation='tanh', use_bias=True)(encoded)\n",
    "    \n",
    "#Define autoencoder models\n",
    "autoencoder = models.Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "autoencoder_encoder = models.Model(input_layer, outputs=encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq6o0QBPateR"
   },
   "source": [
    "---\n",
    "<font color=green>Q27 (1 Mark) :\n",
    "\n",
    "Display all the parameters of the deep neural network.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yLyuzLkGxrAd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">252</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,060</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">252</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,292</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m252\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             â”‚         \u001b[38;5;34m5,060\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m252\u001b[0m)            â”‚         \u001b[38;5;34m5,292\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,352</span> (40.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,352\u001b[0m (40.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,352</span> (40.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,352\u001b[0m (40.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#Q27\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lNBOw3ait03"
   },
   "source": [
    "---\n",
    "<font color=green>Q28: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Train your model using the Adam optimizer for 20 epochs with a batch size equal to 8 and validation split to 20%. Specify the loss function you've chosen.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "IBUtUlxIxsrx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9011 - val_loss: 0.8513\n",
      "Epoch 2/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8592 - val_loss: 0.8192\n",
      "Epoch 3/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8198 - val_loss: 0.7887\n",
      "Epoch 4/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7710 - val_loss: 0.7559\n",
      "Epoch 5/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7437 - val_loss: 0.7216\n",
      "Epoch 6/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7134 - val_loss: 0.6892\n",
      "Epoch 7/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6785 - val_loss: 0.6631\n",
      "Epoch 8/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6611 - val_loss: 0.6415\n",
      "Epoch 9/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6287 - val_loss: 0.6239\n",
      "Epoch 10/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6258 - val_loss: 0.6105\n",
      "Epoch 11/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6030 - val_loss: 0.6012\n",
      "Epoch 12/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5871 - val_loss: 0.5937\n",
      "Epoch 13/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5805 - val_loss: 0.5880\n",
      "Epoch 14/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5675 - val_loss: 0.5829\n",
      "Epoch 15/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5761 - val_loss: 0.5786\n",
      "Epoch 16/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5517 - val_loss: 0.5738\n",
      "Epoch 17/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5613 - val_loss: 0.5697\n",
      "Epoch 18/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5578 - val_loss: 0.5653\n",
      "Epoch 19/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5376 - val_loss: 0.5617\n",
      "Epoch 20/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5317 - val_loss: 0.5595\n"
     ]
    }
   ],
   "source": [
    "#Q28\n",
    "\n",
    "# Train the autoencoder model for 20 epochs with batch size 8 and validation split of 20%\n",
    "history = autoencoder.fit(Z_hat_train.T, Z_hat_train.T, epochs=20, batch_size=8, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xI8O24djAYO"
   },
   "source": [
    "---\n",
    "<font color=green>Q29: (3 Marks) </font>\n",
    "<br><font color='green'>\n",
    "Predict using the testing set and extract the residuals based on the methodology described in **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**.\n",
    "for 'NVDA' stock.\n",
    "</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVDA column index: 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "(27, 20)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "(27, 252)\n",
      "     NVDA Residuals\n",
      "0         -0.015134\n",
      "1         -0.202677\n",
      "2         -0.056248\n",
      "3          0.127158\n",
      "4         -0.475291\n",
      "..              ...\n",
      "247        1.052305\n",
      "248        0.584962\n",
      "249       -1.081445\n",
      "250       -1.437360\n",
      "251        0.017784\n",
      "\n",
      "[252 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the index of the 'NVDA' column\n",
    "nvda_index = Z_hat_test.columns.get_loc('NVDA')\n",
    "print('NVDA column index:', nvda_index)\n",
    "    \n",
    "# Make the prediction by using the autoencoder on the test data and double-check the shapes\n",
    "hidden_factors = autoencoder_encoder.predict(Z_hat_test.T)\n",
    "print(hidden_factors.shape)\n",
    "Z_pred_test = autoencoder.predict(Z_hat_test.T)\n",
    "print(Z_pred_test.shape)\n",
    "\n",
    "# Ensure residuals calculation is correct\n",
    "residuals = Z_hat_test.T.values - Z_pred_test  \n",
    "\n",
    "# Extract residuals for 'NVDA' stock using the correct index\n",
    "nvda_residuals = residuals[nvda_index, :]\n",
    "\n",
    "# Display the residuals for 'NVDA'\n",
    "nvda_residuals_df = pd.DataFrame(nvda_residuals, columns=['NVDA Residuals'])\n",
    "print(nvda_residuals_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "S-EDTK7wxtxB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVDA column index: 0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "(27, 20)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "(27, 252)\n",
      "     NVDA Residuals\n",
      "0         -0.015134\n",
      "1         -0.202677\n",
      "2         -0.056248\n",
      "3          0.127158\n",
      "4         -0.475291\n",
      "..              ...\n",
      "247        1.052305\n",
      "248        0.584962\n",
      "249       -1.081445\n",
      "250       -1.437360\n",
      "251        0.017784\n",
      "\n",
      "[252 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#Q29\n",
    "\n",
    "# Find the index of the 'NVDA' column\n",
    "nvda_index = Z_hat_test.columns.get_loc('NVDA')\n",
    "print('NVDA column index:', nvda_index)\n",
    "    \n",
    "# Make the prediction by using the autoencoder on the test data and double-check the shapes\n",
    "hidden_factors = autoencoder_encoder.predict(Z_hat_test.T)\n",
    "print(hidden_factors.shape)\n",
    "Z_pred_test = autoencoder.predict(Z_hat_test.T)\n",
    "print(Z_pred_test.shape)\n",
    "\n",
    "# Ensure residuals calculation is correct\n",
    "residuals = Z_hat_test.T.values - Z_pred_test\n",
    "\n",
    "# Extract residuals for 'NVDA' stock using the correct index\n",
    "nvda_residuals = residuals[nvda_index, :]\n",
    "\n",
    "# Display the residuals for 'NVDA'\n",
    "nvda_residuals_df = pd.DataFrame(nvda_residuals, columns=['NVDA Residuals'])\n",
    "print(nvda_residuals_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYyVOZu90CIK"
   },
   "source": [
    "<font color=green>Q30: (7 Marks) </font>\n",
    "<br><font color='green'>\n",
    "By reading carrefully the paper **[End-to-End Policy Learning of a Statistical Arbitrage Autoencoder Architecture](https://arxiv.org/pdf/2402.08233.pdf)**, answers the following question:\n",
    "1. **Summarize the Key Actions**: Highlight the main experiments and methodologies employed by the authors in Section 5.\n",
    "2. **Reproduction Steps**: Detail the necessary steps required to replicate the authors' approach based on the descriptions provided in the paper.\n",
    "3. **Proposed Improvement**: Suggest one potential enhancement to the methodology that could potentially increase the effectiveness or efficiency of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na95zQB40hRG"
   },
   "source": [
    "**Summary of Key Actions:** \n",
    "\n",
    "The main idea of section 5 in the paper is to provide a detailed analysis of the steps followed that create the autoencoder strategy.\n",
    "\n",
    "The authors propose the combination of asset pricing models along with statistical arbitrage parameters, that try to maximize the trading performance of the strategy.\n",
    "\n",
    "The Autoencoder takes as an input the standardized stock returns $\\hat{Z}$, uses a one layer encoder to reduce the dimensionality of the data, and then reconstructs the data into the initial dimension using a one layer decoder. \n",
    "\n",
    "$ \\hat{Z}$  = $\\tanh(W^{(1)}(relu(W^{(0)}X + b^{(0)})) + b^{(1)})$\n",
    "\n",
    "\n",
    "Through the encoding phase the authors implement the reLu activation function, while during the decoding phase they apply the tanh activation function. Moreover, they propose the MSE as a Loss Function, using the Adam Optimizer for 10 Epochs. Once they calculate the reconstructed returns, the proceed by calculating the residuals.\n",
    "\n",
    "$\\epsilon_i(t) = Z_i(t) - \\hat{Z}_i(t)$\n",
    "\n",
    "Once, the residuals are calculated, they are then fed into a second neural network, that \"constructs\" portfolio weights, utilizing a hyperbolic tangent activation function, skiping a bias term.\n",
    "\n",
    "$w_t = \\tanh(W^{(2)}(\\hat{Z}_t - Z_t)), \\quad W^{(2)} \\in \\mathbb{R}^{N \\times N}$\n",
    "\n",
    "This second layer, introduces the constraint of Sharpe ratio, so the overall objective becomes to minimize the MSE, while on the same time maximize the Sharpe Ratio, with a parameter Î» = 0.5\n",
    "\n",
    "$\\min_{\\theta} \\left( \\lambda \\text{MSE}(Z_t, \\hat{Z}_t) + (1 - \\lambda) \\text{Sharpe}(w_t, R_{t+1}) \\right)$\n",
    "\n",
    "\n",
    "**Reproduction Steps:**\n",
    "\n",
    "In order to replicate the approach that is described by the authors, we have to follow the following steps:\n",
    "\n",
    "**1) Data Preperation:**\n",
    "The Authors in the paper use daily returns for the US stock market. After the data has been processed, they proceed with calculating the Covariance Matrix over a lookback period of 252 trading days.\n",
    "\n",
    "**2) Autoencoder Structure:**\n",
    "The authors train an autoencoder, with the objective of minimizing the Mean Squared Error.\n",
    "\n",
    "**3) Generating Residuals:**\n",
    "From the reconstructed returns that the autoencoder provides, the authors calculate the residuals.\n",
    "\n",
    "$\\epsilon_i(t) = Z_i(t) - \\hat{Z}_i(t)$\n",
    "\n",
    "**4) Use the Ornstein-Uhlenbeck Process:**\n",
    "Model the residuals, using the OU process, and estimate the model parameters such as $k_i$, $m_i$, and $\\sigma_i$\n",
    "\n",
    "$dX_i(t) = k_i (m_i - X_i(t)) \\, dt + \\sigma_i \\, dW_i(t)$\n",
    "\n",
    "After that, calculate the s-score in order to generate trading signals\n",
    "\n",
    "$$\n",
    "s_i(t) = \\frac{X_i(t) - m_i}{\\sigma_{eq,i}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\sigma_{eq,i} = \\frac{\\sigma_i}{\\sqrt{2k_i}}.\n",
    "$$\n",
    "\n",
    "**5) Generate Trading Signals:**\n",
    "\n",
    "- **Open Long Position**: If $s_i(t) < -1.25$.\n",
    "- **Open Short Position**: If $s_i(t) > 1.25$.\n",
    "- **Close Long Position**: If $s_i(t) > -0.5$.\n",
    "- **Close Short Position**: If $s_i(t) < 0.75$.\n",
    "\n",
    "**6) Construct the portfolio:**\n",
    "Determine the optimal weights by implementing:\n",
    "\n",
    "$w_t = \\tanh(W^{(2)}(\\hat{Z}_t - Z_t))$\n",
    "\n",
    "**7) Optimize the strategy:** Implement the Loss Function:  $\\min_{\\theta} \\left( \\lambda \\text{MSE}(Z_t, \\hat{Z}_t) + (1 - \\lambda) \\text{Sharpe}(w_t, R_{t+1}) \\right)$\n",
    "\n",
    "Train the model for the past 252 days, applying the Adam Optimizer for 10 epoch and a learning rate of 0.001.\n",
    "\n",
    "\n",
    "**Proposed Improvement:**\n",
    "\n",
    "One proposed enhancement by the authors introduces the modeling of transaction costs. Specifically, due to the nature of the model, there exists a high risk from the increased turnover and daily architectural changes.\n",
    "\n",
    "This risk is showcased through the performance of sharpe ratio between the different strategies, since higher turnover increases the risk of the model. Although the Autoencoder delivers exceptionaly better returns compared to other models, if we take into consideration the sharpe ratio we can derive that the strategies perform equally adjusted to risk.\n",
    "\n",
    "An example can be showcased through: AE OU 3 Variant 6  <\n",
    "  PCA OU 8 Factors  â‡”\n",
    "  0.42  <\n",
    "  0.96\n",
    "  \n",
    "One proposed solution to mitigate the impact of transaction costs is to implement penalties for frequent rebalancing. By introducing such penalties, the strategy can be encouraged to make more significant and less frequent adjustments, thereby reducing turnover and associated costs. This approach can help balance the trade-off between optimizing returns and minimizing costs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
